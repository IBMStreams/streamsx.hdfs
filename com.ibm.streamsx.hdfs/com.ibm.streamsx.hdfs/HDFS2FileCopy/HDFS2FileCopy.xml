<?xml version="1.0" encoding="UTF-8"?><operatorModel xmlns="http://www.ibm.com/xmlns/prod/streams/spl/operator" xmlns:cmn="http://www.ibm.com/xmlns/prod/streams/spl/common" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ibm.com/xmlns/prod/streams/spl/operator operatorModel.xsd">
  <!--DO NOT EDIT THIS FILE - it is machine generated-->
  <javaOperatorModel>
    <!--Generated from com.ibm.streamsx.hdfs.HDFS2FileCopy in impl/lib/BigData.jar at Tue Jul 30 15:49:16 CEST 2019-->
    <context>
      <description>The **HDFS2FileCopy** operator copies files from a HDFS fiel system to the loca disk and also from a local disk to the HDFS file system 

The  `HDFS2FileCopy`  uses the hadoop JAVA API functions `copyFromLocalFile` and `copyToLocalFile` to copy files in two directions. 
* `copyFromLocalFile` : Copies a file from local disk to the HDFS file system. 
* `copyToLocalFile`   : Copies a file from HDFS file system to the local disk. 

The recursive copy of directories and subdirectories is not supported

+ Examples 

This example copies the file `test.txt` from local path ./data/ into `/user/hdfs/` directory. 

    streams&lt;boolean succeed&gt; copyFromLocal =  HDFS2FileCopy()
        param
            localFile                : "test.txt"; 
            hdfsFile                 : "/user/hdfs/test.txt"; 
            deleteSourceFile         : false; 
            overwriteDestinationFile : true; 
            direction                : copyFromLocalFile
    }

This example copies all files from the local path `/tmp/work` into HDFS directory `/user/hdfs/work` . 
    // DirectoryScan operator with an absolute directory argument. 
    stream&lt;rstring localFile&gt; DirScan = DirectoryScan() 
    {  
        param 
            directory       : "/tmp/work"; 
            initDelay       : 1.0; 
    } 

    // Copies all incoming files from input port into /user/hdfs/work directory. 
    stream&lt;rstring message, uint64 elapsedTime&gt; CopyFromLocal = HDFS2FileCopy(DirScan)
    { 
        param
              hdfsUser                 : "hdfs"; 
              hdfsFile                 : "/user/hdfs/work/"; 
              deleteSourceFile         : false; 
              overwriteDestinationFile : true; 
              direction                : copyFromLocalFile; 
              localFileAttrName        : "localFile"; 
    }

This example copies all files from the HDFS directory `/user/hdfs/work` into the local directory `/tmp/work2` . 

You have to perform the follwing steps for kerberos configuration:* 1- Copy the kerberos keytab file of your hdfs user from  HDFS server into etc directory. 
* 2- Replace the kerberos principal with your hdsf principal. 
* 3- Copy the `core-site.xml` file from your HDFS server into `etc` directory. 
* 4- Copy the kerberos configuration file in `/etc` directory of your streams server. 


    //`HDFS2DirectoryScan` operator with an absolute directory argument and with kerberos authentication 

    // parameters returns all HDFS file names. 
    stream&lt;rstring hdfsFile&gt; HdfsDirScan = HDFS2DirectoryScan() 
    {  
        param 
            configPath               : "etc1"; 
            authKeytab               : "etc/hdfs.headless.keytab"; 
            authPrincipal            : "hdfs-hdpcluster@HDP2.COM"; 
            vmArg                    : "-Djava.security.krb5.conf=/etc/krb5.conf"; 
            directory                : "/user/hdfs/work"; 
            sleepTime                : 2.0; 
    } 

    // CopyToLocal copies all incoming HDFS files from input port into local directory /tmp/work2 . 
    stream&lt;rstring message, uint64 elapsedTime&gt; CopyToLocal = HDFS2FileCopy(HdfsDirScan) 
    { 
        param
            authKeytab               : "etc/hdfs.headless.keytab"; 
            authPrincipal            : "hdfs-hdpcluster@HDP2.COM"; 
            configPath               : "etc1"; 
            vmArg                    : "-Djava.security.krb5.conf=/etc/krb5.conf"; 
            hdfsFileAttrName         : "hdfsFile"; 
            deleteSourceFile         : false; 
            overwriteDestinationFile : true; 
            direction                : copyToLocalFile; 
            localFile                : "/tmp/work2"; 
    }
</description>
      <iconUri size="16">HDFS2FileCopy_16.gif</iconUri>
      <iconUri size="32">HDFS2FileCopy_32.gif</iconUri>
      <metrics/>
      <customLiterals>
        <enumeration>
          <name>com.ibm.streamsx.hdfs.HDFS2FileCopy.copyDirection</name>
          <value>copyFromLocalFile</value>
          <value>copyToLocalFile</value>
        </enumeration>
      </customLiterals>
      <executionSettings>
        <className>com.ibm.streamsx.hdfs.HDFS2FileCopy</className>
      </executionSettings>
      <libraryDependencies>
        <library>
          <cmn:description>Operator class library</cmn:description>
          <cmn:managedLibrary>
            <cmn:libPath>../../impl/lib/BigData.jar</cmn:libPath>
          </cmn:managedLibrary>
        </library>
      </libraryDependencies>
    </context>
    <parameters>
      <parameter>
        <name>authKeytab</name>
        <description>This parameter specifies the file that contains the encrypted keys for the user that is specified by the **authPrincipal** parameter. 
The operator uses this keytab file to authenticate the user. 
The keytab file is generated by the administrator.  You must specify this parameter to use Kerberos authentication.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>authPrincipal</name>
        <description>This parameter specifies the Kerberos principal that you use for authentication. 
This value is set to the principal that is created for the IBM Streams instance owner. 
You must specify this parameter if you want to use Kerberos authentication.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>configPath</name>
        <description>This parameter specifies the path to the directory that contains the `core-site.xml` file, which is an HDFS
configuration file. If this parameter is not specified, by default the operator looks for the `core-site.xml` file in the following locations:
* `$HADOOP_HOME/etc/hadoop`
* `$HADOOP_HOME/conf`
* `$HADOOP_HOME/lib` 
* `$HADOOP_HOME/`
**Note:** For connections to Hadoop instances deployed on IBM Analytics Engine, the `$HADOOP_HOME` environment variable is not supported and should not be used.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>credFile</name>
        <description>This parameter specifies a file that contains login credentials. The credentials are used to connect to GPFS remotely by using the `webhdfs://hdfshost:webhdfsport` schema.  The credentials file must contain information about how to authenticate with IBM Analytics Engine when using the webhdfs schema. 
For example, the file must contain the user name and password for an IBM Analytics Engine user. 
When connecting to HDFS instances deployed on IBM Analytics Engine, 
the credentials are provided using the **hdfsUser** and **hdfsPassword** parameters.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>deleteSourceFile</name>
        <description>This optional parameter specifies whether to delete the source file when processing is finished.</description>
        <optional>true</optional>
        <type>boolean</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>direction</name>
        <description>This parameter specifies the direction of copy. The parameter can be set with the following values. 
* `copyFromLocalFile`  Copy a file from local disk to the HDFS file system. 
* `copyToLocalFile` Copy a file from HDFS file system to the local disk. 
</description>
        <optional>false</optional>
        <expressionMode>CustomLiteral</expressionMode>
        <type>com.ibm.streamsx.hdfs.HDFS2FileCopy.copyDirection</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>hdfsFile</name>
        <description>This optional parameter specifies the name of HDFS file or directory. 
If the name starts with a slash, it is considered an absolute path of HDFS file that you want to use. 
If it does not start with a slash, it is considered a relative path, relative to the '/user/*userid*/hdfsFile. 
If you want to copy all incoming files from input port to a directory set the value of direction to `copyFromLocalFile` and set the value of this parameter to a directory with a slash at the end e.g.  '/user/userid/testDir/. 
This parameter is mandatory if the 'hdfsFileAttrNmae' is not specified in input port. 
`hdfsFile` cannot be set when parameter `hdfsFileAttrName` is set. 
</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>hdfsFileAttrName</name>
        <description>This optional parameter specifies the value of hdfsFile that coming through input stream. 
If the name starts with a slash, it is considered an absolute path of HDFS file that you want to copy. 
If it does not start with a slash, it is considered a relative path, relative to the '/user/*userid*/hdfsFile. 
This parameter is mandatory if the 'hdfsFile' is not specified. 
`hdfsFileAttrName` cannot be set when parameter `hdfsFile` is set.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>hdfsPassword</name>
        <description>This parameter specifies the password to use when you connecting to a Hadoop instance deployed on IBM Analytics Engine. 
If this parameter is not specified, attempts to connect to a Hadoop instance deployed on IBM Analytics Engine will cause an exception.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>hdfsUri</name>
        <description>This parameter specifies the uniform resource identifier (URI) that you can use to connect to 
the HDFS file system.  The URI has the following format:
* To access HDFS locally or remotely, use `hdfs://hdfshost:hdfsport` 
* To access GPFS locally, use `gpfs:///`. 
* To access GPFS remotely, use `webhdfs://hdfshost:webhdfsport`. 
* To access HDFS via a web connection for HDFS deployed on IBM Analytics Engine, use `webhdfs://webhdfshost:webhdfsport`. 

If this parameter is not specified, the operator expects that the HDFS URI is specified as the `fs.defaultFS` or `fs.default.name` property in the `core-site.xml` HDFS configuration file.  The operator expects the `core-site.xml` 
file to be in `$HADOOP_HOME/../hadoop-conf` or `$HADOOP_HOME/etc/hadoop`  or in the directory specified by the **configPath** parameter. 
**Note:** For connections to HDFS on IBM Analytics Engine, the `$HADOOP_HOME` environment variable is not supported and so either  **hdfsUri** or **configPath**  must be specified.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>hdfsUser</name>
        <description>This parameter specifies the user ID to use when you connect to the HDFS file system. 
If this parameter is not specified, the operator uses the instance owner ID to connect to HDFS. 
When connecting to Hadoop instances on IBM Analytics Engine, this parameter must be specified otherwise the connection will be unsuccessful. 
When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by using the 
values that are specified in the **authPrincipal** and **authKeytab** parameters.  After successful authentication, the 
operator uses the user ID that is specified by the **hdfsUser** parameter to perform all other operations on the file system.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>keyStorePassword</name>
        <description>This optional parameter is only supported when connecting to a Hadoop instance deployed on IBM Analytics Engine. 
It specifies the password for the keystore file. This attribute is specified when the **keyStore** attribute is specified and the keystore file is protected by a password. 
If the keyStorePassword is invalid the operator terminates.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>keyStorePath</name>
        <description>This optional parameter is only supported when connecting to a Hadoop instance deployed on IBM Analytics Engine. 
It specifies the path to the keystore file, which is in PEM format. The keystore file is used when making a secure connection to the HDFS server and must contain the public certificate of the HDFS server that will be connected to. 
**Note: If this parameter is omitted, invalid certificates for secure connections will be accepted.**  If the keystore file does not exist, or if the certificate it contains is invalid, the operator terminates.. 
The location of the keystore file can be absolute path on the filesystem or a path that is relative to the application directory. 
See the section on 'SSL Configuration' in the main page of this toolkit's documentation for information on how to configure the keystore. 
The location of the keystore file can be absolute path on the filesystem or a path that is relative to the application directory.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>libPath</name>
        <description>This optional parameter specifies the absolute path to the directory that contains the Hadoop library files. 
If this parameter is omitted and `$HADOOP_HOME` is not set, the apache hadoop specific libraries within the `impl/lib/ext` folder of the toolkit will be used. 
When specified, this parameter takes precedence over the `$HADOOP_HOME` environment variable and the libraries within the folder indicated by `$HADOOP_HOME` will not be used.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>localFile</name>
        <description>This optional parameter specifies the name of local file to be copied. 
If the name starts with a slash, it is considered an absolute path of local file that you want to copy. 
If it does not start with a slash, it is considered a relative path, relative to your project data directory. 
If you want to copy all incoming files from input port to a directory set the value of direction to `copyToLocalFile` and  set the value of this parameter to a directory with a slash at the end e.g.  'data/testDir/. 
This parameter is mandatory if the 'localFileAttrNmae' is not specified in input port. 
`localFile` cannot be set when parameter `localFileAttrName` is set. 
</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>localFileAttrName</name>
        <description>This optional parameter specifies the value of localFile that coming through input stream. 
If the name starts with a slash, it is considered an absolute path of local file that you want to copy. 
If it does not start with a slash, it is considered a relative path, relative to your project data directory. 
This parameter is mandatory if the 'localFile' is not specified. 
`localFileAttrName` cannot be set when parameter `localFile` is set. 
</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>overwriteDestinationFile</name>
        <description>This optional parameter specifies whether to overwrite the destination file.</description>
        <optional>true</optional>
        <type>boolean</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>policyFilePath</name>
        <description>This optional parameter is relevant when connecting to IBM Analytics Engine on IBM Cloud. 
It specifies the path to the directory that contains the Java Cryptography Extension policy files (US_export_policy.jar and local_policy.jar). 
The policy files enable the Java operators to use encryption with key sizes beyond the limits specified by the JDK. 
See the section on 'Policy file configuration' in the main page of this toolkit's documentation for information on how to configure the policy files. 
If this parameter is omitted the JVM default policy files will be used. When specified, this parameter takes precedence over the JVM default policy files. 

**Note:** This parameter changes a JVM property. If you set this property, be sure it is set to the same value in all HDFS operators that are in the same PE. 
The location of the policy file directory can be absolute path on the file system or a path that is relative to the application directory.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>reconnectionBound</name>
        <description>This optional parameter specifies the number of successive connection attempts that occur when a connection fails or a disconnect occurs. 
It is used only when the **reconnectionPolicy** parameter is set to `BoundedRetry`; otherwise, it is ignored. The default value is `5`.</description>
        <optional>true</optional>
        <type>int32</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>reconnectionInterval</name>
        <description>This optional parameter specifies the amount of time (in seconds) that the operator waits between successive connection attempts. 
It is used only when the **reconnectionPolicy** parameter is set to `BoundedRetry` or `InfiniteRetry`; othewise, it is ignored.  The default value is `10`.</description>
        <optional>true</optional>
        <type>float64</type>
        <cardinality>1</cardinality>
      </parameter>
      <parameter>
        <name>reconnectionPolicy</name>
        <description>This optional parameter specifies the policy that is used by the operator to handle HDFS connection failures. 
The valid values are: `NoRetry`, `InfiniteRetry`, and `BoundedRetry`. The default value is `BoundedRetry`. 
If `NoRetry` is specified and a HDFS connection failure occurs, the operator does not try to connect to the HDFS again. 
The operator shuts down at startup time if the initial connection attempt fails. 
If `BoundedRetry` is specified and a HDFS connection failure occurs, the operator tries to connect to the HDFS again up to a maximum number of times. 
The maximum number of connection attempts is specified in the **reconnectionBound** parameter.  The sequence of connection attempts occurs at startup time. 
If a connection does not exist, the sequence of connection attempts also occurs before each operator is run. 
If `InfiniteRetry` is specified, the operator continues to try and connect indefinitely until a connection is made. 
This behavior blocks all other operator operations while a connection is not successful. 
For example, if an incorrect connection password is specified in the connection configuration document, the operator remains in an infinite startup loop until a shutdown is requested.</description>
        <optional>true</optional>
        <type>rstring</type>
        <cardinality>1</cardinality>
      </parameter>
    </parameters>
    <inputPorts>
      <inputPortSet>
        <description>The `HDFS2FileCopy` operator has one input port, which contents the file names that you specified. 
The input port is non-mutating, and its punctuation mode is `Oblivious` &gt;. 

The schema of the input port is:

`tuple&lt;rstring localFileAttrName, rstring hdfsFileAttrName&gt;`

or one of them:
`tuple&lt;rstring localFileAttrName&gt;` or `tuple&lt;rstring hdfsFileAttrName&gt;` , which specifies a rstring for file names. 
</description>
        <windowingDescription/>
        <windowingMode>NonWindowed</windowingMode>
        <windowPunctuationInputMode>Oblivious</windowPunctuationInputMode>
        <controlPort>false</controlPort>
        <cardinality>1</cardinality>
        <optional>true</optional>
      </inputPortSet>
    </inputPorts>
    <outputPorts>
      <outputPortSet>
        <description>The `HDFS2FileCopy` operator is configurable with an optional output port. 
The output port is non-mutating and its punctuation mode is `Free`. 

The schema of the output port is:

 `&lt;string result, uint64 elapsedTime&gt;` 

, which delivers the result of copy process and the elapsed time in milisecunds. 

In case of any error it returns the error message as result</description>
        <windowPunctuationOutputMode>Free</windowPunctuationOutputMode>
        <cardinality>1</cardinality>
        <optional>true</optional>
      </outputPortSet>
    </outputPorts>
  </javaOperatorModel>
</operatorModel>