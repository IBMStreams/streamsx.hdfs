<?xml version="1.0" encoding="UTF-8"?>
<operatorModel xmlns="http://www.ibm.com/xmlns/prod/streams/spl/operator" xmlns:cmn="http://www.ibm.com/xmlns/prod/streams/spl/common" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.ibm.com/xmlns/prod/streams/spl/operator operatorModel.xsd">
    <javaOperatorModel>
        <context>
            <description docHref="doc/${nl}/spldoc/html/tk$com.ibm.streamsx.hdfs/op$com.ibm.streamsx.hdfs$HDFS2FileSink.html">The `HDFS2FileSink` operator writes files to a Hadoop Distributed File System. 

The `HDFS2FileSink` operator is similar to the `FileSink` operator.  
This operator writes tuples that arrive on its input port to the output file that is named by the **file** parameter.  
You can optionally control whether the operator closes the current output file and creates a new file for writing based on the size 
of the file in bytes, the number of tuples that are written to the file, or the time in seconds that the file is open for writing, 
or when the operator receives a punctuation marker.

# Behavior in a consistent region

The `HDFS2FileSink` operator can participate in a consistent region, however this is not supported when connecting to IBM Analytics Engine on IBM Cloud.
The operator can be part of a consistent region, but cannot be at the start of a consistent region.
The operator guarantees that tuples are written to a file in HDFS at least once,
but duplicated tuples can be written to the file if application failure occurs.

For the operator to support consistent region, the Hadoop Distributed File System must be configured with file append
enabled. For information about how to properly enable this feature, refer to the documentation of your Hadoop distribution.

On drain, the operator flushes its internal buffer to the file.
On checkpoint, the operator stores the current file name, file size, tuple count, and file number to the checkpoint.
On reset, the operator closes the current file, and opens the file from checkpoint.
File states like file size and tuple count are reset to the file.
The file is opened in append mode, and data is written to the end of the file. 

# Exceptions

The `HDFS2FileSink` operator terminates in the following cases:
 * The operator cannot connect to HDFS.
 * The file cannot be written.     

+ Examples

This is a basic example using the `HDFS2FileSink` operator to write output to a Hadoop filesystem deployed on IBM Cloud (IBM Analytics Engine).
    () as Sink= HDFS2FileSink(Input){
      param
        file   :"/user/clsadmin/myfile.txt";
        hdfsUri:"webhdfs://server_host_name:port";   
        hdfsUser: "clsadmin";
        hdfsPassword: "IAEPassword";
    }   
    
    
This example uses the `HDFS2FileSink` operator to write tuples to output files that have names like `output0.txt`. SSL certificate validation is enabled.

    stream&lt;PersonSchema&gt; In = FileSource(){
      param
        file : "Input.txt" ;
    }
    stream&lt;rstring PersonSchemString&gt; SingleStringIn = Functor(In){
      output
        SingleStringIn : PersonSchemString =(rstring) In ;
    }
    () as txtSink = HDFS2FileSink(SingleStringIn){
      param
        file : "output%FILENUM.txt" ;
        bytesPerFile: (int64)(16*1024);
 		hdfsUri: "webhdfs://server_host_name:port";   
        hdfsUser: "clsadmin";
        hdfsPassword: "IAEPassword";
        keyStorePassword: "storepass";
        keyStorePath: "etc/store.jks";        
    }</description>
            <iconUri size="32">HDFS2FileSink_32.gif</iconUri>
            <iconUri size="16">HDFS2FileSink_16.gif</iconUri>
            <executionSettings>
                <className>com.ibm.streamsx.hdfs.HDFS2FileSink</className>
                <vmArgs />
            </executionSettings>
            <libraryDependencies>
                <library>
                    <cmn:description>Java operator class library</cmn:description>
                    <cmn:managedLibrary>
                        <cmn:libPath>../../impl/java/bin</cmn:libPath>
                        <cmn:libPath>../../impl/lib/BigData.jar</cmn:libPath>
                        <cmn:command />
                    </cmn:managedLibrary>
                </library>
            </libraryDependencies>
            <codeTemplates>
                <codeTemplate name="Basic HDFS2FileSink template">
                    <description sampleUri="">Basic HDFS2FileSink template</description>
                    <template>() as ${operatorName} = HDFS2FileSink(${inputStream} ) {
            param
                file: "${filename}";
        }</template>
                </codeTemplate>
                <codeTemplate name="HDFS2FileSink template for IAE on IBM Cloud">
                    <description sampleUri="">HDFS2FileSink template for IAE on IBM Cloud</description>
                    <template>() as ${operatorName} = HDFS2FileSink(${inputStream} ) {
            param
                file: "${filename}";
                hdfsUser: "${hdfsUser}";
                hdfsUri: "${hdfsUri}";
                hdfsPassword: "${hdfsPassword}";
                
        }</template>
                </codeTemplate>
            </codeTemplates>
        </context>
        <parameters>
            <description />
            <parameter>
                <name>file</name>
                <description>This parameter specifies the name of the file that the operator writes to.           
The **file** parameter can optionally contain the following variables, which the operator evaluates at runtime to generate the file name:
 * %HOST 		The host that is running the processing element (PE) of this operator.
 * %FILENUM		The file number, which starts at 0 and counts up as a new file is created for writing.
 * %PROCID		The process ID of the processing element.
 * %PEID 		The processing element ID.
 * %PELAUNCHNUM	The PE launch count.
 * %TIME 		The time when the file is created.  If the **timeFormat** parameter is not specified, the default time format is `yyyyMMdd_HHmmss`.
 
For example, if you specify a **file** parameter of `myFile%FILENUM%TIME.txt`, and the first three files are created in the afternoon on November 30, 2014, 
the file names are `myFile020141130_132443.txt`, `myfile120141130_132443.txt`, and `myFile220141130_132443.txt`.

**Important:** If the %FILENUM specification is not included, the file is overwritten every time a new file is created.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
   
            <parameter>
               <name>tempFile</name>
               <description>
This parameter specifies the name of the file that the operator writes to. When the file is closed the file is renamed to the final filename defined by the **file** parameter or **fileAttributeName** parameter.
The **tempFile** parameter can optionally contain the following variables, which the operator evaluates at runtime to generate the file name:
 * %HOST 		The host that is running the processing element (PE) of this operator.
 * %PROCID		The process ID of the processing element.
 * %PEID 		The processing element ID.
 * %PELAUNCHNUM	The PE launch count.
 * %TIME 		The time when the file is created.  If the **timeFormat** parameter is not specified, the default time format is `yyyyMMdd_HHmmss`.
**Important:** This parameter must not be used in a consistent region.
</description>
             <optional>true</optional>
             <type>rstring</type>
              <cardinality>1</cardinality>
            </parameter>
            
            
            
            <parameter>
                <name>timeFormat</name>
                <description>This parameter specifies the time format to use when the **file** parameter value contains `%TIME`.  
The parameter value must contain conversion specifications that are supported by the java.text.SimpleDateFormat. 
The default format is `yyyyMMdd_HHmmss`.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>hdfsUri</name>
                <description>This parameter specifies the uniform resource identifier (URI) that you can use to connect to the HDFS file system.  
The URI has the following format:
 * To access HDFS locally or remotely, use `hdfs://*hdfshost*:*hdfsport*`.
 * To access GPFS locally, use `gpfs:///`.
 * To access GPFS remotely, use `webhdfs://*hdfshost*:*webhdfsport*`.
 * To access HDFS via a web connection for HDFS deployed on IBM Cloud, use `webhdfs://webhdfshost:webhdfsport`. 
If this parameter is not specified, the operator expects that the HDFS URI is specified as the `fs.defaultFS` or `fs.default.name` 
property in the `core-site.xml` HDFS configuration file.  The operator expects the `core-site.xml` file to be located in 
`$HADOOP_HOME/../hadoop-conf` or `$HADOOP_HOME/etc/hadoop` or in the directory specified by the **configPath** parameter. 

You can use the **hdfsUri** parameter to override the value that is specified for the `fs.defaultFS` or `fs.default.name` option
in the `core-site.xml` configuration file.
**Note:** For applications connecting to HDFS deployed on IBM Cloud, the `$HADOOP_HOME` environment variable is not supported so either  **hdfsUri** or **configPath**  must be specified.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>bytesPerFile</name>
                <description>This parameter specifies the approximate size of the output file, in bytes. 
When the file size exceeds the specified number of bytes, the current output file is closed and a new file is opened.  
The **bytesPerFile**, **timePerFile**, and **tuplesPerFile** parameters are mutually exclusive; you can specify only one of these parameters at a time.</description>
                <optional>true</optional>
                <type>int64</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>tuplesPerFile</name>
                <description>This parameter specifies the maximum number of tuples that can be received for each output file. 
When the specified number of tuples are received, the current output file is closed and a new file is opened for writing. 
The **bytesPerFile**, **timePerFile**, and **tuplesPerFile** parameters are mutually exclusive; you can specify only one of these parameters at a time.</description>
                <optional>true</optional>
                <type>int64</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>closeOnPunct</name>
                <description>This parameter specifies whether the operator closes the current output file and creates a new file when a punctuation marker is received. 
The default value is `false`.</description>
                <optional>true</optional>
                <type>boolean</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>fileAttributeName</name>
                <description>If set, this points to an attribute containing the filename.  The operator will close a file when value of this attribute changes.  If the string contains substitutions, the check for a change happens before substituations, and the filename contains the substitutions based on the first tuple.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>timePerFile</name>
                <description>This parameter specifies the approximate time, in seconds, after which the current output file is closed and a new file is opened for writing.  
The **bytesPerFile**, **timePerFile**, and **tuplesPerFile** parameters are mutually exclusive; you can specify only one of these parameters.</description>
                <optional>true</optional>
                <type>float64</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>hdfsUser</name>
                <description>This parameter specifies the user ID to use when you connect to the HDFS file system.  
If this parameter is not specified, the operator uses the instance owner ID to connect to the HDFS file system.  
When connecting to Hadoop instances on IBM Cloud, this parameter must be specified otherwise the connection will be unsuccessful.
When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by 
using the values that are specified in the **authPrincipal** and **authKeytab** parameters.  After successful authentication,
the operator uses the user ID that is specified in the **hdfsUser** parameter to perform all other operations on the file system.

**NOTE:** When you use Kerberos authentication, the IBM Streams instance owner must have super user privileges on HDFS or GPFS
to perform operations as the user that is specified by the **hdfsUser** parameter.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>hdfsPassword</name>
                <description>This parameter specifies the password to use when you connecting to a Hadoop instance deployed on IBM Cloud.  
If this parameter is not specified, attempts to connect to a Hadoop instance deployed on IBM Cloud will cause an exception.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>reconnectionPolicy</name>
                <description>This optional parameter specifies the policy that is used by the operator to handle HDFS connection failures.  The valid values are: `NoRetry`, `InfiniteRetry`, and `BoundedRetry`. The default value is `BoundedRetry`. If `NoRetry` is specified and a HDFS connection failure occurs, the operator does not try to connect to the HDFS again.  The operator shuts down at startup time if the initial connection attempt fails. If `BoundedRetry` is specified and a HDFS connection failure occurs, the operator tries to connect to the HDFS again up to a maximum number of times. The maximum number of connection attempts is specified in the **reconnectionBound** parameter.  The sequence of connection attempts occurs at startup time. If a connection does not exist, the sequence of connection attempts also occurs before each operator is run.  If `InfiniteRetry` is specified, the operator continues to try and connect indefinitely until a connection is made.  This behavior blocks all other operator operations while a connection is not successful.  For example, if an incorrect connection password is specified in the connection configuration document, the operator remains in an infinite startup loop until a shutdown is requested.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>reconnectionBound</name>
                <description>This optional parameter specifies the number of successive connection attempts that occur when a connection fails or a disconnect occurs.  It is used only when the **reconnectionPolicy** parameter is set to `BoundedRetry`; otherwise, it is ignored. The default value is `5`.</description>
                <optional>true</optional>
                <type>int32</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>reconnectionInterval</name>
                <description>This optional parameter specifies the amount of time (in seconds) that the operator waits between successive connection attempts.  It is used only when the **reconnectionPolicy**  parameter is set to `BoundedRetry` or `InfiniteRetry`; othewise, it is ignored.  The default value is `10`.</description>
                <optional>true</optional>
                <type>float64</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>keyStorePath</name>
                <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on IBM Cloud. It specifies the path to the keystore file, which is in PEM format. The keystore file is used when making a secure connection to the HDFS server and must contain the public certificate of the HDFS server.  See the section on "SSL Configuration" in the main page of this toolkit's documentation for information on how to configure the keystore.
**Note: If this parameter is omitted, invalid certificates for secure connections will be accepted.**  If the keystore file does not exist, or if the certificate it contains is invalid, the operator terminates. The location of the keystore file can be absolute path on the file system or a path that is relative to the application directory.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>keyStorePassword</name>
                <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on IBM Cloud. It specifies the password for the keystore file. This attribute is specified when the **keyStore** attribute is specified and the keystore file is protected by a password. If the keyStorePassword is invalid the operator terminates.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>authPrincipal</name>
                <description>This parameter specifies the Kerberos principal that you use for authentication. 
This value is set to the principal that is created for the IBM Streams instance owner.
You must specify this parameter if you want to use Kerberos authentication.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>authKeytab</name>
                <description>This parameter specifies the file that contains the encrypted keys for the user that is specified by the **authPrincipal** parameter.
The operator uses this keytab file to authenticate the user.  The keytab file is generated by the administrator.
You must specify this parameter to use Kerberos authentication.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>credFile</name>
                <description>This parameter specifies the file that contains the login credentials. 
These credentials are used when you connect to GPFS remotely by using the `webhdfs://*hdfshost*:*hdfsport*` schema.
The credentials file must contain information on how to authenticate with IBM Analytics Engine when using the webhdfs schema.
For example, the file must contain the user name and password for an IBM Analytics Engine user.
When connecting to HDFS instances deployed on IBM Cloud, the credentials are provided using the **hdfsUser** and **hdfsPassword** parameters.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>configPath</name>
                <description>This parameter specifies the absolute path to the configuration directory that contains the `core-site.xml` file. 
If this parameter is not specified, by default the operator looks for the `core-site.xml` file in the following locations:
 * `$HADOOP_HOME/../hadoop-conf`
 * `$HADOOP_HOME/etc/hadoop`
 * `$HADOOP_HOME/conf`
 * `$HADOOP_HOME/share/hadoop/hdfs/\*`
 * `$HADOOP_HOME/share/hadoop/common/\*`
 * `$HADOOP_HOME/share/hadoop/common/lib/\*`
 * `$HADOOP_HOME/lib/\*`
 * `$HADOOP_HOME/\*`
You can use the **hdfsUri** parameter to override the value that is specified for the `fs.defaultFS` or `fs.default.name` option in the `core-site.xml` configuration file.
 **Note:** For connections to HDFS on IBM Cloud, the `$HADOOP_HOME` environment variable is not supported and so either  **hdfsUri** or **configPath**  must be specified.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>encoding</name>
                <description>This parameter specifies the character set encoding that is used in the output ﬁle.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>libPath</name>
                <description>This optional parameter specifies the absolute path to the directory that contains the Hadoop library files. If this parameter is omitted and `$HADOOP_HOME` is not set, the Cloud specific libraries within the `impl/lib/ext` folder of the toolkit will be used. 
When specified, this parameter takes precedence over the `$HADOOP_HOME` environment variable and the libraries within the folder indicated by `$HADOOP_HOME` will not be used.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
            <parameter>
                <name>policyFilePath</name>
                <description>This optional parameter is relevant when connecting to IBM Analytics Engine on IBM Cloud. It specifies the path to the directory that contains the Java Cryptography Extension policy files (US_export_policy.jar and local_policy.jar). The policy files enable the Java operators to use encryption with key sizes beyond the limits specified by the JDK. See the section on "Policy file configuration" in the main page of this toolkit's documentation for information on how to configure the policy files.
If this parameter is omitted the JVM default policy files will be used. When specified, this parameter takes precedence over the JVM default policy files.
**Note:** This parameter changes a JVM property. If you set this property, be sure it is set to the same value in all HDFS operators that are in the same PE. The location of the policy file directory can be absolute path on the file system or a path that is relative to the application directory.</description>
                <optional>true</optional>
                <type>rstring</type>
                <cardinality>1</cardinality>
            </parameter>
        </parameters>
        <inputPorts>
            <inputPortSet>
                <description docHref="" sampleUri="">The `HDFS2FileSink` operator has one input port, which writes the contents of the input stream to the file that you specified.
The input port is non-mutating, and its punctuation mode is `Oblivious`.  The HDFS2FileSink supports writing data into HDFS in two formats.
For line format, the schema of the input port is tuple&lt;rstring line&gt; or tuple&lt;ustring line&gt;, which specifies a single rstring or ustring attribute that represents 
a line to be written to the file. For binary format, the schema of the input port is tuple&lt;blob data&gt;, which specifies a block of data to be written to the file.</description>
                <windowingDescription />
                <windowingMode>NonWindowed</windowingMode>
                <windowPunctuationInputMode>Oblivious</windowPunctuationInputMode>
                <cardinality>1</cardinality>
                <optional>false</optional>
            </inputPortSet>
        </inputPorts>
        <outputPorts>
            <outputPortSet>
                <description>The `HDFS2FileSink` operator is configurable with an optional output port.  
The output port is non-mutating and its punctuation mode is `Free`.
The schema of the output port is &lt;string fileName, uint64 fileSize&gt;, which specifies the name and size of files that are written to HDFS.</description>
                <windowPunctuationOutputMode>Free</windowPunctuationOutputMode>
                <windowPunctuationInputPort>-1</windowPunctuationInputPort>
                <cardinality>1</cardinality>
                <optional>true</optional>
            </outputPortSet>
        </outputPorts>
    </javaOperatorModel>
</operatorModel>

