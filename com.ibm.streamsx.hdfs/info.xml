<?xml version="1.0" encoding="UTF-8"?>
<!--
// *******************************************************************************
// * Copyright (C)2014 2018, International Business Machines Corporation and *
// * others. All Rights Reserved. *
// *******************************************************************************
-->
<toolkitInfoModel xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo" xsi:schemaLocation="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo toolkitInfoModel.xsd">
  <identity>
    <name>com.ibm.streamsx.hdfs</name>
    <description>
The HDFS Toolkit provides operators that can read and write data from Hadoop Distributed File System (HDFS) version 2 or later.

The operators in this toolkit use Hadoop Java APIs to access HDFS and GPFS. The operators support the following versions of Hadoop distributions:
 * Apache Hadoop versions 2.7 
 * Cloudera distribution including Apache Hadoop version 4 (CDH4) and version 5 (CDH 5)
 * Hortonworks Data Platform (HDP) Version 2.6 or higher

Note: The reference platforms that were used for testing are Hadoop 2.7.3, HDP .

You can access GPFS remotely by specifying the `webhdfs://hdfshost:webhdfsport` schema in the URI that you use to connect to GPFS.
for example:
 
      {  
		     param
			      hdfsUri "clsadmin": 
			      hdfsUser: "webhdfs://your-hdfs-host-ip-address:8443";
			      hdfsPassword: "PASSWORD";
                  ...
                  
      }
    
 Or "hdfs://your-hdfs-host-ip-address:8020" as hdfsUri
              
      {  
		     param
			      hdfsUri "hdfs": 
			      hdfsUser: "hdfs://your-hdfs-host-ip-address:8020"
                  ...
      }
             
              
              
For Apache Hadoop 2.x, CDH, and HDP, you can optionally configure these operators to use the Kerberos protocol to authenticate users that read and write to HDFS. Kerberos authentication provides a more secure way of accessing HDFS by providing user authentication. To use Kerberos authentication, you must configure the **authPrincipal** and **authKeytab** operator parameters at compile time. The **authPrincipal** parameter specifies the Kerberos principal, which is typically the principal that is created for the Streams instance owner. The **authKeytab** parameter specifies the keytab file that is created for the principal.


+ Developing and running applications that use the HDFS Toolkit

To create applications that use the HDFS Toolkit, you must configure either Streams Studio
or the SPL compiler to be aware of the location of the toolkit. 

# Before you begin

* Install IBM InfoSphere Streams.  Configure the product environment variables by entering the following command: 
      source product-installation-root-directory/streams-version/bin/streamsprofile.sh

# About this task

After the location of the toolkit is communicated to the compiler, the SPL artifacts that are specified
in the toolkit can be used by an application.
The application can include a use directive to bring the necessary namespaces into scope.
Alternatively, you can fully qualify the operators that are provided by toolkit with their namespaces as prefixes.

# Procedure

1. If InfoSphere Streams has access to the impl/lib/ext where Hadoop libraries located.
   The JAR libraries will be download via maven. 
   The new version of HDFS toolkit need not necessarily a Hadoop client installation.

2. Build your application.  You can use the **sc** command or Streams Studio.  
3. Start the InfoSphere Streams instance. 
4. Run the application. You can submit the application as a job by using the **streamtool submitjob** command or by using Streams Studio. 
 
</description>
    <version>4.0.0</version>
    <requiredProductVersion>4.1.0.0</requiredProductVersion>
  </identity>
  <dependencies/>
</toolkitInfoModel>

