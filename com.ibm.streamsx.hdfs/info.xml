<?xml version="1.0" encoding="UTF-8"?>
<!--
// *******************************************************************************
// * Copyright (C)2014 2018, International Business Machines Corporation and *
// * others. All Rights Reserved. *
// *******************************************************************************
-->
<toolkitInfoModel xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo" xsi:schemaLocation="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo toolkitInfoModel.xsd">
  <identity>
    <name>com.ibm.streamsx.hdfs</name>
    <description>
The HDFS Toolkit provides operators that can read and write data from Hadoop Distributed File System (HDFS) version 2 or later.

The operators in this toolkit use Hadoop Java APIs to access HDFS and GPFS. The operators support the following versions of Hadoop distributions:
 * Apache Hadoop versions 2.x 
 * HDP 2.7.x
 * Cloudera distribution including Apache Hadoop version 4 (CDH4) and version 5 (CDH 5)
 * Hortonworks Data Platform (HDP) 2.2

Note: The reference platforms that were used for testing are Hadoop 2.7.3, HDP .

You can access GPFS remotely by specifying the `webhdfs://hdfshost:webhdfsport` schema in the URI that you use to connect to GPFS.

For Apache Hadoop 2.x, CDH, and HDP, you can optionally configure these operators to use the Kerberos protocol to authenticate users that read and write to HDFS. Kerberos authentication provides a more secure way of accessing HDFS by providing user authentication. To use Kerberos authentication, you must configure the **authPrincipal** and **authKeytab** operator parameters at compile time. The **authPrincipal** parameter specifies the Kerberos principal, which is typically the principal that is created for the Streams instance owner. The **authKeytab** parameter specifies the keytab file that is created for the principal.


+ Developing and running applications that use the HDFS Toolkit

To create applications that use the HDFS Toolkit, you must configure either Streams Studio
or the SPL compiler to be aware of the location of the toolkit. 

# Before you begin

* Install IBM InfoSphere Streams.  Configure the product environment variables by entering the following command: 
      source product-installation-root-directory/4.2.0.0/bin/streamsprofile.sh
* Install a supported version of Hadoop.
* Ensure that InfoSphere Streams has access to Hadoop libraries and configuration files
  to allow streams processing applications to read and write to HDFS.

# About this task

After the location of the toolkit is communicated to the compiler, the SPL artifacts that are specified
in the toolkit can be used by an application.
The application can include a use directive to bring the necessary namespaces into scope.
Alternatively, you can fully qualify the operators that are provided by toolkit with their namespaces as prefixes.

# Procedure

# Procedure

1. If InfoSphere Streams has access to the location where Hadoop is installed, 
   set the following environment variables:
   * For Apache HDFS, Cloudera, or Hortonworks Data Platform:
     * Set **HADOOP_HOME** to `Hadoop_Install_Directory`. For example, `/usr/lib/hadoop`.
     * Set **JAVA_HOME** to the location where Java is installed.
2. To read and write to HDFS, specify a uniform resource identifier (URI) to connect to HDFS.
   You can specify the URI in one of the following ways:
   * Specify a value for the `fs.defautlFS` or `fs.default.name` option in the `core-site.xml` HDFS configuration file.
     By default, the operators look for the `core-site.xml` file in the following directories:
     * `$HADOOP_HOME/../hadoop-conf`
     * `$HADOOP_HOME/etc/hadoop`
     * `$HADOOP_HOME/conf`
     * `$HADOOP_HOME/share/hadoop/hdfs/\*`
     * `$HADOOP_HOME/share/hadoop/common/\*`
     * `$HADOOP_HOME/share/hadoop/common/lib/\*`
     * `$HADOOP_HOME/lib/\*`
     * `$HADOOP_HOME/\*`
     Tip: To specify a different location for the HDFS configuration files, set the **configPath** operator parameter.
   * Specify a value for the **hdfsUri** operator parameter.
7. Build your application.  You can use the **sc** command or Streams Studio.  
8. Start the InfoSphere Streams instance. 
9. Run the application. You can submit the application as a job by using the **streamtool submitjob** command or by using Streams Studio. 

</description>
    <version>3.1.0</version>
    <requiredProductVersion>4.1.0.0</requiredProductVersion>
  </identity>
  <dependencies/>
</toolkitInfoModel>
