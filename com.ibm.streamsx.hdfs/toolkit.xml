<?xml version="1.0" encoding="UTF-8" standalone="no" ?>
<toolkitModel xmlns="http://www.ibm.com/xmlns/prod/streams/spl/toolkit" productVersion="4.2.5.0" xmlns:common="http://www.ibm.com/xmlns/prod/streams/spl/common" xmlns:ti="http://www.ibm.com/xmlns/prod/streams/spl/toolkitInfo" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">

  <toolkit name="com.ibm.streamsx.hdfs" requiredProductVersion="4.1.0.0" version="4.0.0">
    <description>
The HDFS Toolkit for Bluemix enhances the operators in the standard HDFS toolkit to support read and write operations to HDFS running in the BigInsights service on Bluemix.
The operators in this toolkit use Hadoop Java APIs to access HDFS and have been tested on IBM BigInsights 4.1.

This toolkit allows you to access remote HDFS servers on Bluemix, which require user authentication using a username and password.

If you are accessing GPFS, you do not need to install InfoSphere Streams on an InfoSphere BigInsights data node. Instead, you can access GPFS remotely by specifying the `webhdfs://hdfshost:webhdfsport` schema in the URI that you use to connect to GPFS.

For Apache Hadoop 2.x, CDH, and HDP, you can optionally configure these operators to use the Kerberos protocol to authenticate users that read and write to HDFS. Kerberos authentication provides a more secure way of accessing HDFS by providing user authentication. To use Kerberos authentication, you must configure the **authPrincipal** and **authKeytab** operator parameters at compile time. The **authPrincipal** parameter specifies the Kerberos principal, which is typically the principal that is created for the Streams instance owner. The **authKeytab** parameter specifies the keytab file that is created for the principal.

Restriction: Kerberos authentication is not supported for InfoSphere BigInsights 2.1.2 and 3.0.0.x, 4.0.0.x or on BigInsights on Bluemix.


+ Using the toolkit to connect to HDFS in the BigInsights on Bluemix service

To create applications that use the HDFS Toolkit for Bluemix, you must configure either Streams Studio or the SPL compiler to be aware of the location of the toolkit. 

# Before you begin

* Install IBM InfoSphere Streams.  Configure the product environment variables by entering the following command: 
      source product-installation-root-directory/4.1.0.0/bin/streamsprofile.sh

# About this task

After the location of the toolkit is communicated to the compiler, the SPL artifacts that are specified
in the toolkit can be used by an application.
The application can include a use directive to bring the necessary namespaces into scope.
Alternatively, you can fully qualify the operators that are provided by toolkit with their namespaces as prefixes.

**Important Note**

The HDFS for Bluemix toolkit depends on specific Hadoop libraries and these have been included in the `impl/lib/ext` folder.
Thus, unlike the standard HDFS toolkit, the **HADOOP_HOME** environment variable should *not* be used to specify the location of the Hadoop libraries.
If the  **HADOOP_HOME** environment variable is set, attempts to connect to a HDFS server on Bluemix will be unsuccessful. 


# Procedure

1. Set the following environment variables:
     * Set **JAVA_HOME** to the location where Java is installed.
2. Configure the SPL compiler to find the toolkit root directory. Use one of the following methods:
  * Set the **STREAMS_SPLPATH** environment variable to the root directory of a toolkit
    or multiple toolkits (with : as a separator).  For example:
      export STREAMS_SPLPATH=$STREAMS_INSTALL/toolkits/com.ibm.streamsx.hdfs
  * Specify the **-t** or **--spl-path** command parameter when you run the **sc** command. For example:
      sc -t $STREAMS_INSTALL/toolkits/com.ibm.streamsx.hdfs -M MyMain
    where MyMain is the name of the SPL main composite.
    **Note**: These command parameters override the **STREAMS_SPLPATH** environment variable.
  * Add the toolkit location in InfoSphere Streams Studio.
3. Develop your application. To avoid the need to fully qualify the operators, add a use directive in your application. 
  * For example, you can add the following clause in your SPL source file:
      use com.ibm.streamsx.hdfs::*;
    You can also specify a use clause for individual operators by replacing the asterisk (\*) with the operator name. For example: 
      use com.ibm.streamsx.hdfs::HDFS2FileSink;
4. SSL Configuration

   By default, when connecting to HDFS on Bluemix, the server certificate is accepted without validation. If you decide to enable SSL certificate validation for the connection to Bluemix, complete the following steps.
   * Follow these [https://www.ng.bluemix.net/docs/services/BigInsights/index-gentopic1.html#exportcerttotruststore|instructions] to export the SSL certificate and add it to your truststore.  Note that you should export the certificate from the  IBM BigInsights Home page, and not from the Ambari console.  The BigInsights home page is typically at `https://server_host_name:8443/gateway/default/BigInsightsWeb/index.html`.   
   * Copy the truststore into a location accessible by your application directory, for example, &lt;application_directory&gt;/etc/mystore.jks.
   * Set the **keyStorePath** and **keyStorePassword** parameters in your application to the path to your keystore and the keystore password respectively. For example:
		 () as HDFS2FileSink_2 = HDFS2FileSink(In){
		     param
		      file : "output_file.txt" ;
		      hdfsUri : "webhdfs://server_host_name:8443" ;
		      keyStorePassword:"storepass";
		      keyStorePath: "etc/store.jks";
		      hdfsPassword: "bi_password";
		      hdfsUser:"bi_user";
		   }
5. Policy file configuration

   BigInsights on Bluemix uses encryption keys larger than what is supported by the default IBM JDK. To connect to BigInsights, complete the following steps.
   * Download and extract the [https://www-01.ibm.com/marketing/iwm/iwm/web/preLogin.do?source=jcesdk|Unrestricted SDK JCE policy files]. You have two options when deciding where to put the US_export_policy.jar and local_policy.jar:
     * If you have access to the Streams cluster file system, you can replace the default policy files in &lt;JAVA_HOME&gt;/jre/lib/security/ with the ones you downloaded.
     * If you cannot access the file system, put them somewhere in the application's etc directory - for example, &lt;application_directory&gt;/etc/policyfiles/ .
   * If you placed the policy files in your application's etc directory, set the **policyFilePath** parameter in your application to the relative path to the directory containing your policy files. For example:
		 () as HDFS2FileSink_2 = HDFS2FileSink(In) {
		     param
		      file: "output_file.txt";
		      hdfsUri: "webhdfs://server_host_name:8443";
		      hdfsPassword: "bi_password";
		      hdfsUser: "bi_user";
		      policyFilePath: "etc/policyfiles/"
		 }
6. To read and write to HDFS, specify a uniform resource identifier (URI) to connect to HDFS using the **hdfsUri** operator parameter.
7. Build your application.  You can use the **sc** command or Streams Studio.  
8. Start the InfoSphere Streams instance. 
9. Run the application. You can submit the application as a job by using the **streamtool submitjob** command or by using Streams Studio. You can also submit the application to run in the cloud using the Streaming Analytics service on Bluemix.

+ Using the toolkit to connect to HDFS outside of Bluemix

These instructions apply if you are not connecting to HDFS in Bluemix.  
To create applications that use the HDFS Toolkit, you must configure either Streams Studio or the SPL compiler to be aware of the location of the toolkit. 

# Before you begin

* Install IBM InfoSphere Streams.  Configure the product environment variables by entering the following command: 
      source product-installation-root-directory/4.1.0.0/bin/streamsprofile.sh
* Install a supported version of Hadoop. 
* Ensure that InfoSphere Streams has access to Hadoop libraries and configuration files  to allow streams processing applications to read and write to HDFS.   

# About this task

After the location of the toolkit is communicated to the compiler, the SPL artifacts that are specified
in the toolkit can be used by an application.
The application can include a use directive to bring the necessary namespaces into scope.
Alternatively, you can fully qualify the operators that are provided by toolkit with their namespaces as prefixes.


# Procedure

1. If InfoSphere Streams has access to the location where Hadoop is installed, 
   set the following environment variables:
   * For Apache HDFS, Cloudera, or Hortonworks Data Platform:
     * Set **HADOOP_HOME** to `Hadoop_Install_Directory`. For example, `/usr/hdp/hadoop`.
     * Set **JAVA_HOME** to the location where Java is installed.
2. Configure the SPL compiler to find the toolkit root directory. Use one of the following methods:
  * Set the **STREAMS_SPLPATH** environment variable to the root directory of a toolkit
    or multiple toolkits (with : as a separator).  For example:
      export STREAMS_SPLPATH=$STREAMS_INSTALL/toolkits/com.ibm.streamsx.hdfs
  * Specify the **-t** or **--spl-path** command parameter when you run the **sc** command. For example:
      sc -t $STREAMS_INSTALL/toolkits/com.ibm.streamsx.hdfs -M MyMain
    where MyMain is the name of the SPL main composite.
    **Note**: These command parameters override the **STREAMS_SPLPATH** environment variable.
  * Add the toolkit location in InfoSphere Streams Studio.
4. Develop your application. To avoid the need to fully qualify the operators, add a use directive in your application. 
  * For example, you can add the following clause in your SPL source file:
      use com.ibm.streamsx.hdfs::*;
    You can also specify a use clause for individual operators by replacing the asterisk (\*) with the operator name. For example: 
      use com.ibm.streamsx.hdfs::HDFS2FileSink;
6. To read and write to HDFS, specify a uniform resource identifier (URI) to connect to HDFS.
   You can specify the URI in one of the following ways:
   * Specify a value for the `fs.defautlFS` or `fs.default.name` option in the `core-site.xml` HDFS configuration file.
     By default, the operators look for the `core-site.xml` file in the following directories:
     * `$HADOOP_HOME/../hadoop-conf`
     * `$HADOOP_HOME/etc/hadoop`
     * `$HADOOP_HOME/conf`
     * `$HADOOP_HOME/share/hadoop/hdfs/\*`
     * `$HADOOP_HOME/share/hadoop/common/\*`
     * `$HADOOP_HOME/share/hadoop/common/lib/\*`
     * `$HADOOP_HOME/lib/\*`
     * `$HADOOP_HOME/\*`
     Tip: To specify a different location for the HDFS configuration files, set the **configPath** operator parameter.
   * Specify a value for the **hdfsUri** operator parameter.
7. Build your application.  You can use the **sc** command or Streams Studio.  
8. Start the InfoSphere Streams instance. 
9. Run the application. You can submit the application as a job by using the **streamtool submitjob** command or by using Streams Studio. 

</description>
    <uriTable>
      <uri index="7" value="com.ibm.streamsx.hdfs/HDFS2FileSink"/>
      <uri index="6" value="com.ibm.streamsx.hdfs/HDFS2FileSource/HDFS2FileSource_16.gif"/>
      <uri index="3" value="com.ibm.streamsx.hdfs/HDFS2DirectoryScan/HDFS2DirectoryScan_16.gif"/>
      <uri index="1" value="com.ibm.streamsx.hdfs/HDFS2DirectoryScan"/>
      <uri index="8" value="com.ibm.streamsx.hdfs/HDFS2FileSink/HDFS2FileSink_32.gif"/>
      <uri index="2" value="com.ibm.streamsx.hdfs/HDFS2DirectoryScan/HDFS2DirectoryScan_32.gif"/>
      <uri index="9" value="com.ibm.streamsx.hdfs/HDFS2FileSink/HDFS2FileSink_16.gif"/>
      <uri index="5" value="com.ibm.streamsx.hdfs/HDFS2FileSource/HDFS2FileSource_32.gif"/>
      <uri index="4" value="com.ibm.streamsx.hdfs/HDFS2FileSource"/>
    </uriTable>
    <namespace name="com.ibm.streamsx.hdfs">
      <primitiveOp language="Java" modelUriIndex="1" name="HDFS2DirectoryScan" public="true">
        <description docHref="doc/${nl}/spldoc/html/tk$com.ibm.streamsx.hdfs/op$com.ibm.streamsx.hdfs$HDFS2DirectoryScan.html">
The `HDFS2DirectoryScan` operator scans a Hadoop Distributed File System directory for new or modified files.

The `HDFS2DirectoryScan` is similar to the `DirectoryScan` operator. 
The `HDFS2DirectoryScan` operator repeatedly scans an HDFS directory and writes the names of new or modified files 
that are found in the directory to the output port. The operator sleeps between scans.

# Behavior in a consistent region

 The `HDFS2DirectoryScan` operator can participate in a consistent region.
 The operator can be at the start of a consistent region if there is no input port.
 The operator supports periodic and operator-driven consistent region policies.
 
 If consistent region policy is set as operator driven, the operator initiates a drain after each tuple is submitted.
 This allows for a consistent state to be established after a file is fully processed.
 If consistent region policy is set as periodic, the operator respects the period setting and establishes consistent states accordingly.
 This means that multiple files can be processed before a consistent state is established.
 
 At checkpoint, the operator saves the last submitted filename and its modification timestamp to the checkpoint.
 Upon application failures, the operator resubmits all files that are newer than the last submitted file at checkpoint.

# Exceptions

The operator terminates in the following cases:
 * The operator cannot connect to HDFS.
 * The **strictMode** parameter is true but the directory is not found.
 * The path that is given by the directory name exists, but is an ordinary file and not a directory.
 * HDFS failed to give a list of files in the directory.
 * The pattern that is specified in the pattern parameter fails to compile.

+ Examples

This example uses the `HDFS2DirectoryScan` operator to scan the HDFS directory on Bluemix.  The **hdfsUser** and **hdfsPassword** parameters are used to provide the username and password for authentication.
	
	(stream&lt;rstring filename&gt; Files) as HDFS2DirectoryScan_1 = HDFS2DirectoryScan()
	{
		param
			directory :"/user/myuser/";
			hdfsUri   :"webhdfs://hdfsServer:8443";
			hdfsPassword: "password";
			hdfsUser: "biuser";
			sleepTime : 2.0;
	}

This example uses the `HDFS2DirectoryScan` operator to scan a HDFS directory every two seconds. 
The **hdfsUri** parameter in this case overrides the value that is specified by the `fs.defaultFS` option in the `core-site.xml`.

	
	(stream&lt;rstring filename&gt; Files) as HDFS2DirectoryScan_1 = HDFS2DirectoryScan()
	{
		param
			directory :"/user/myuser/";
			hdfsUri   :"hdfs://hdfsServer:1883";
			sleepTime : 2.0;
	}	
 </description>
        <images>
          <image size="32" uriIndex="2"/>
          <image size="16" uriIndex="3"/>
        </images>
        <parameter expressionMode="Constant" name="vmArg" optional="true" type="rstring">
          <description>
Specifies command-line arguments that are passed to the Java virtual machine that the operator runs within.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="directory" optional="true" type="rstring">
          <description>This optional parameter specifies the name of the directory to be scanned. If the name starts with a slash, it is considered an absolute directory that you want to scan. If it does not start with a slash, it is considered a relative directory, relative to the '/user/*userid*/ directory. This parameter is mandatory if the input port is not specified.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="initDelay" optional="true" type="float64">
          <description sampleUri="">This optional parameter specifies the time to wait in seconds before the operator scans the directory for the first time. The default value is 0.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="sleepTime" optional="true" type="float64">
          <description sampleUri="">This optional parameter specifies the minimum time between directory scans. The default value is 5.0 seconds. </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="pattern" optional="true" type="rstring">
          <description sampleUri="">
This optional parameter limits the file names that are listed to the names that match the specified regular expression. 
The `HDFS2DirectoryScan` operator ignores file names that do not match the specified regular expression. </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="strictMode" optional="true" type="boolean">
          <description sampleUri="">This optional parameter determines whether the operator reports an error if the directory to be scanned does not exist. If you set this parameter to true and the specified directory does not exist or there is a problem accessing the directory, the operator reports an error and terminates. If you set this parameter to false and the specified directory does not exist or there is a problem accessing the directory, the operator treats it as an empty directory and does not report an error. </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsUri" optional="true" type="rstring">
          <description>This optional parameter of type rstring specifies the uniform resource identifier (URI) that you can use to connect to HDFS. The URI defines the host and the port that you can use to connect to HDFS.  The URI has the following format:
 * To access HDFS locally or remotely, use `hdfs://hdfshost:hdfsport` 
 * To access GPFS locally, use `gpfs:///` 
 * To access GPFS remotely, use `webhdfs://hdfshost:webhdfsport` 
 * To access HDFS via a web connection for HDFS deployed on Bluemix, use `webhdfs://webhdfshost:webhdfsport`. 
If this parameter is not specified, the operator finds the HDFS URI from the `fs.defaultFS` or `fs.default.name` option in the `core-site.xml` HDFS configuration file. The path to the folder containing the `core-site.xml` can be set using the **configPath** parameter.  Otherwise, the file is expected to be located in one of the following locations:
 * `$HADOOP_HOME/../hadoop-conf`
 * `$HADOOP_HOME/etc/hadoop` 
 * `$HADOOP_HOME/conf` 
 * `$HADOOP_HOME/share/hadoop/hdfs/\*` 
 * `$HADOOP_HOME/share/hadoop/common/\*` 
 * `$HADOOP_HOME/share/hadoop/common/lib/\*` 
 * `$HADOOP_HOME/lib/\*` 
 * `$HADOOP_HOME/\*` 
**Note:** For connections to Hadoop instances deployed on Bluemix, the `$HADOOP_HOME` environment variable is not supported and so either the **hdfsUri** or the **configPath** parameter must be specified.

</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsPassword" optional="true" type="rstring">
          <description>
This parameter specifies the password to use when you connecting to a Hadoop instance deployed on Bluemix.  
If this parameter is not specified, attempts to connect to a Hadoop instance deployed on Bluemix will cause an exception.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsUser" optional="true" type="rstring">
          <description>This optional parameter specifies the user ID to use when you connect to HDFS.  If this parameter is not specified, the operator uses the instance owner ID to connect to HDFS. 
When connecting to Hadoop instances on Bluemix, this parameter must be specified otherwise the connection will be unsuccessful.
When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by using the values specified by the **authPrincipal** and **authKeytab** parameters. After successful authentication, the operator uses the user ID that is specified by the **hdfsUser** parameter to perform all other operations on the file system.

Note: When using Kerberos authentication, to perform operations as the user specified by the **hdfsUser** parameter, the InfoSphere Streams instance owner must have super user privileges on HDFS or GPFS.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="keyStorePath" optional="true" type="rstring">
          <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on Bluemix. It specifies the path to the keystore file, which is in PEM format. The keystore file is used when making a secure connection to the HDFS server and must contain the public certificate of the HDFS server that will be connected to. 
See the section on "SSL Configuration" in the main page of this toolkit's documentation for information on how to configure the keystore. **Note: If this parameter is omitted, invalid certificates for secure connections will be accepted**. If the keystore file does not exist, or if the certificate it contains is invalid, the operator terminates. 
The location of the keystore file can be absolute path on the file system or a path that is relative to the application directory.
        </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="keyStorePassword" optional="true" type="rstring">
          <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on Bluemix. It specifies the password for the keystore file. This attribute is specified when the **keyStore** attribute is specified and the keystore file is protected by a password. If the keyStorePassword is invalid the operator terminates.
        </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="authPrincipal" optional="true" type="rstring">
          <description>This optional parameter specifies the Kerberos principal that you use for authentication. This value is set to the principal that is created for the InfoSphere Streams instance owner. You must specify this parameter if you want to use Kerberos authentication.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="authKeytab" optional="true" type="rstring">
          <description>This parameter specifies the file that contains the encrypted keys for the user that is specified by the **authPrincipal** parameter. The operator uses this keytab file to authenticate the user. The keytab file is generated by the administrator. You must specify this parameter to use Kerberos authentication.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="credFile" optional="true" type="rstring">
          <description>This optional parameter contains the login credentials to use when you connect to GPFS remotely by using the `webhdfs://hdfshost:webhdfsport` schema. The credentials file must contain information on how to authenticate with IBM InfoSphere BigInsights when using the webhdfs schema. For example, the file must contain the user name and password for an IBM InfoSphere BigInsights user. 
When connecting to HDFS instances deployed on Bluemix, the credentials are provided using the **hdfsUser** and **hdfsPassword** parameters.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="policyFilePath" optional="true" type="rstring">
          <description>
This optional parameter is relevant when connecting to BigInsights on Bluemix. It specifies the path to the directory that contains the Java Cryptography Extension policy files (US_export_policy.jar and local_policy.jar). The policy files enable the Java operators to use encryption with key sizes beyond the limits specified by the JDK. See the section on "Policy file configuration" in the main page of this toolkit's documentation for information on how to configure the policy files.
If this parameter is omitted the JVM default policy files will be used. When specified, this parameter takes precedence over the JVM default policy files.
**Note:** This parameter changes a JVM property. If you set this property, be sure it is set to the same value in all HDFS operators that are in the same PE. The location of the policy file directory can be absolute path on the file system or a path that is relative to the application directory.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="configPath" optional="true" type="rstring">
          <description>This optional parameter specifies the absolute path to the directory that contains the HDFS configuration files. If this parameter is not specified, by default the operator looks for the `core-site.xml` file in the following locations:
 * `$HADOOP_HOME/../hadoop-conf`
 * `$HADOOP_HOME/etc/hadoop`
 * `$HADOOP_HOME/conf`
 * `$HADOOP_HOME/share/hadoop/hdfs/\*`
 * `$HADOOP_HOME/share/hadoop/common/\*`
 * `$HADOOP_HOME/share/hadoop/common/lib/\*`
 * `$HADOOP_HOME/lib/\*`
 * `$HADOOP_HOME/\*`
 
You can use the **hdfsUri** parameter to override the value that is specified for the `fs.defaultFS` or `fs.default.name` option in the `core-site.xml` configuration file.
 **Note:** For connections to Hadoop instances deployed on Bluemix, the `$HADOOP_HOME` environment variable is not supported and should not be used.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="libPath" optional="true" type="rstring">
          <description>This optional parameter specifies the absolute path to the directory that contains the Hadoop library files. If this parameter is omitted and `$HADOOP_HOME` is not set, the Bluemix specific libraries within the `impl/lib/ext` folder of the toolkit will be used. 
When specified, this parameter takes precedence over the `$HADOOP_HOME` environment variable and the libraries within the folder indicated by `$HADOOP_HOME` will not be used.
</description>
        </parameter>
        <inputPort maxNumPorts="1" minNumPorts="1" optional="true" windowPunctInputMode="Oblivious">
          <description>
The `HDFS2DirectoryScan` operator has an optional control input port. You can use this port to change the directory 
that the operator scans at run time without restarting or recompiling the application.  
The expected schema for the input port is of tuple&lt;rstring directory&gt;, a schema containing a single attribute of type rstring.  
If a directory scan is in progress when a tuple is received, the scan completes and a new scan starts immediately after 
and uses the new directory that was specified.  
If the operator is sleeping, the operator starts scanning the new directory immediately after it receives an input tuple.</description>
          <windowPolicy>NonWindowed</windowPolicy>
        </inputPort>
        <outputPort expressionMode="Nonexistent" maxNumPorts="1" minNumPorts="1" optional="false" windowPunctOutputMode="Free">
          <description>
The `HDFS2DirectoryScan` operator has one output port. 
This port provides tuples of type rstring that are encoded in UTF-8 and represent the file names that are found in the directory, 
one file name per tuple.  The file names do not occur in any particular order. 
The port is non-mutating and punctuation free.</description>
        </outputPort>
        <codeTemplate name="HDFS2DirectoryScan" template="stream&lt;rstring name> ${outputStream} = HDFS2DirectoryScan()   {&#xA;            param&#xA;                directory: &quot;${directoryToScan}&quot;;&#xA;        } ">
          <description>Basic use of HDFS Directory Scan</description>
        </codeTemplate>
        <codeTemplate name="HDFS2DirectoryScan with HDFS2FileSource connecting to Bluemix" template="stream&lt;rstring name> ${outputStream} = HDFS2DirectoryScan()   {&#xA;            param&#xA;                directory: &quot;${directoryToScan}&quot;;&#xA;                hdfsUri   :&quot;{webhdfs://hdfsServer:8443&quot;};&#xA;&#x9;&#x9;&#x9;&#x9;hdfsPassword: &quot;{password&quot;};&#xA;&#x9;&#x9;&#x9;&#x9;hdfsUser: &quot;{biuser&quot;};&#xA;        }&#xA;        &#xA;        stream&lt;${schema}> ${fileSourceStream} = HDFS2FileSource(${outputStream})   {&#xA;        &#x9;param&#xA;&#x9;&#x9;&#x9;&#x9;hdfsUri&#x9;:&quot;{webhdfs://hdfsServer:8443}&quot;;&#xA;&#x9;&#x9;&#x9;&#x9;hdfsPassword&#x9;:&quot;{password}&quot;;&#xA;&#x9;&#x9;&#x9;&#x9;hdfsUser&#x9;:&quot;{user}&quot;;&#xA;        }">
          <description>Read all the files in an HDFS directory using the HDFS2FileSource operator</description>
        </codeTemplate>
      </primitiveOp>
      <primitiveOp language="Java" modelUriIndex="4" name="HDFS2FileSource" public="true">
        <description docHref="doc/${nl}/spldoc/html/tk$com.ibm.streamsx.hdfs/op$com.ibm.streamsx.hdfs$HDFS2FileSource.html">
The `HDFS2FileSource` operator reads files from a Hadoop Distributed File System (HDFS).

The operator opens a file on HDFS and sends out its contents in tuple format on its output port.  

If the optional input port is not specified, the operator reads the HDFS file that is specified in the **file** parameter and 
provides the file contents on the output port.  If the optional input port is configured, the operator reads the files that are 
named by the attribute in the tuples that arrive on its input port and places a punctuation marker between each file.

# Behavior in a consistent region

The `HDFS2FileSource` operator can participate in a consistent region.
The operator can be at the start of a consistent region if there is no input port.

The operator supports periodic and operator-driven consistent region policies.
If the consistent region policy is set as operator driven, the operator initiates a drain after a file is fully read.
If the consistent region policy is set as periodic, the operator respects the period setting and establishes consistent states accordingly.
This means that multiple consistent states can be established before a file is fully read.

At checkpoint, the operator saves the current file name and file cursor location.
If the operator does not have an input port, upon application failures, the operator resets
the file cursor back to the checkpointed location, and starts replaying tuples from the cursor location.
If the operator has an input port and is in a consistent region, the operator relies on its upstream operators
to properly reply the filenames for it to re-read the files from the beginning.

# Exceptions

The `HDFS2FileSource` operator terminates in the following cases:
 * The operator cannot connect to HDFS.
 * The file cannot be opened.
 * The file does not exist.
 * The file becomes unreadable.
 * A tuple cannot be created from the file contents (such as a problem with the file format).

+ Examples

This example uses the `HDFS2DirectoryScan` operator to scan the HDFS directory every two seconds and the `HDFS2FileSource`
operator to read the files that are output by the `HDFS2DirectoryScan` operator.
   
	/*
    * HDFS2DirectoryScan operator scans /user/myser/ directory from HDFS every 2.0 seconds
    */
    (stream&lt;rstring filename&gt; Files) as HDFS2DirectoryScan_1 = HDFS2DirectoryScan(){
      param
        directory     : "/user/myuser/";
        hdfsUri:"hdfs : //hdfsServer:1883";
        hdfsUser: "biadmin";
        hdfsPassword: "Password";
        sleepTime     : 2.0;
    }
	// HDFS2FileSource operator reads from files discovered by HDFS2DirectoryScan operator
	//If the **keyStorePath** and **keyStorePassword** are omitted,  the operator will accept all certificates as valid
	
	(stream&lt;rstring data&gt; FileContent) as HDFS2FileSource_2 =	HDFS2FileSource(Files){
	  param
	    hdfsUri:"hdfs://hdfsSever:1883";
	    hdfsUser: "biadmin";
        hdfsPassword: "Password";
	 }


The following example shows the operator configured to access a HDFS instance on Bluemix to read a file specified by the *file* parameter.  The **hdfsUser** and **hdfsPassword** are the username and password that have access to the Hadoop instance.

    stream&lt;rstring data&gt; FileContent) as HDFS2FileSource_2 = HDFS2FileSource(){
      param
        hdfsUri:"webhdfs://server_host_name:port";
        file   :"/user/biadmin/myfile.txt";
        hdfsUser: "biadmin";
        hdfsPassword: "Password";
        keyStorePassword: "storepass";
        keyStorePath: "etc/store.jks";
    }

</description>
        <images>
          <image size="32" uriIndex="5"/>
          <image size="16" uriIndex="6"/>
        </images>
        <parameter expressionMode="Constant" name="vmArg" optional="true" type="rstring">
          <description>
Specifies command-line arguments that are passed to the Java virtual machine that the operator runs within.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="file" optional="true" type="rstring">
          <description>
This parameter specifies the name of the file that the operator opens and reads. 
This parameter must be specified when the optional input port is not configured.
If the optional input port is used and the file name is specified, the operator generates an error.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="initDelay" optional="true" type="float64">
          <description sampleUri="">
This parameter specifies the time to wait in seconds before the operator reads the first file.  
The default value is `0`.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsUri" optional="true" type="rstring">
          <description>
This parameter specifies the uniform resource identifier (URI) that you can use to connect to 
the HDFS file system.  The URI has the following format:
 * To access HDFS locally or remotely, use `hdfs://hdfshost:hdfsport`
 * To access GPFS locally, use `gpfs:///`.
 * To access GPFS remotely, use `webhdfs://hdfshost:webhdfsport`.
 * To access HDFS via a web connection for HDFS deployed on Bluemix, use `webhdfs://webhdfshost:webhdfsport`.

If this parameter is not specified, the operator expects that the HDFS URI is specified as the `fs.defaultFS` or 
`fs.default.name` property in the `core-site.xml` HDFS configuration file.  The operator expects the `core-site.xml` 
file to be in `$HADOOP_HOME/../hadoop-conf` or `$HADOOP_HOME/etc/hadoop`  or in the directory specified by the **configPath** parameter.
**Note:** For connections to HDFS on Bluemix, the `$HADOOP_HOME` environment variable is not supported and so either  **hdfsUri** or **configPath**  must be specified.
 </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsUser" optional="true" type="rstring">
          <description>
This parameter specifies the user ID to use when you connect to the HDFS file system. If this parameter is not specified, the operator uses the instance owner ID to connect to HDFS.
When connecting to Hadoop instances on Bluemix, this parameter must be specified otherwise the connection will be unsuccessful. 
When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by using the 
values that are specified in the **authPrincipal** and **authKeytab** parameters.  After successful authentication, the
operator uses the user ID that is specified by the **hdfsUser** parameter to perform all other operations on the file system.

**NOTE:** When using Kerberos authentication, the InfoSphere Streams instance owner must have super user privileges on HDFS or GPFS
to perform operations as the user that is specified by the **hdfsUser** parameter.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsPassword" optional="true" type="rstring">
          <description>
This parameter specifies the password to use when you connecting to a Hadoop instance deployed on Bluemix.  
If this parameter is not specified, attempts to connect to a Hadoop instance deployed on Bluemix will cause an exception.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="keyStorePath" optional="true" type="rstring">
          <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on Bluemix. It specifies the path to the keystore file, which is in PEM format. The keystore file is used when making a secure connection to the HDFS server and must contain the public certificate of the HDFS server that will be connected to. 
**Note: If this parameter is omitted, invalid certificates for secure connections will be accepted.**  If the keystore file does not exist, or if the certificate it contains is invalid, the operator terminates. The location of the keystore file can be absolute path on the filesystem or a path that is relative to the application directory.  See the section on "SSL Configuration" in the main page of this toolkit's documentation for information on how to configure the keystore.
The location of the keystore file can be absolute path on the filesystem or a path that is relative to the application directory.
        </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="keyStorePassword" optional="true" type="rstring">
          <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on Bluemix. It specifies the password for the keystore file. This attribute is specified when the **keyStore** attribute is specified and the keystore file is protected by a password. If the keyStorePassword is invalid the operator terminates.
        </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="authPrincipal" optional="true" type="rstring">
          <description>
This parameter specifies the Kerberos principal that you use for authentication.
This value is set to the principal that is created for the InfoSphere Streams instance owner.
You must specify this parameter if you want to use Kerberos authentication.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="authKeytab" optional="true" type="rstring">
          <description>
This parameter specifies the file that contains the encrypted keys for the user that is specified
by the **authPrincipal** parameter.  The operator uses this keytab file to authenticate the user.
The keytab file is generated by the administrator.  You must specify this parameter to use Kerberos authentication.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="credFile" optional="true" type="rstring">
          <description>
This parameter specifies a file that contains login credentials. The credentials are used to
connect to GPFS remotely by using the `webhdfs://hdfshost:webhdfsport` schema.  The credentials file must contain information
about how to authenticate with IBM InfoSphere BigInsights when using the webhdfs schema.  For example, the file must contain the 
user name and password for an IBM InfoSphere BigInsights user.  When connecting to HDFS instances deployed on Bluemix, the credentials are provided using the **hdfsUser** and **hdfsPassword** parameters.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="configPath" optional="true" type="rstring">
          <description>
This parameter specifies the absolute path to the directory that contains the `core-site.xml` file, which is an HDFS
configuration file. If this parameter is not specified, by default the operator looks for the `core-site.xml` file in the following locations:
 * `$HADOOP_HOME/../hadoop-conf`
 * `$HADOOP_HOME/etc/hadoop`
 * `$HADOOP_HOME/conf`
 * `$HADOOP_HOME/share/hadoop/hdfs/\*`
 * `$HADOOP_HOME/share/hadoop/common/\*`
 * `$HADOOP_HOME/share/hadoop/common/lib/\*`
 * `$HADOOP_HOME/lib/\*`
 * `$HADOOP_HOME/\*`
**Note:** For connections to Hadoop instances deployed on Bluemix, the `$HADOOP_HOME` environment variable is not supported and should not be used.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="encoding" optional="true" type="rstring">
          <description>This parameter specifies the encoding to use when reading files. The default value is `UTF-8`.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="blockSize" optional="true" type="int32">
          <description>
		This parameter specifies the maximum number of bytes to be read at one time when reading a file into binary mode (ie, into a blob); thus, it is the maximum size of the blobs on the output stream. The parameter is optional, and defaults to `4096`.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="libPath" optional="true" type="rstring">
          <description>
This optional parameter specifies the absolute path to the directory that contains the Hadoop library files. If this parameter is omitted and `$HADOOP_HOME` is not set, the Bluemix specific libraries within the `impl/lib/ext` folder of the toolkit will be used. 
When specified, this parameter takes precedence over the `$HADOOP_HOME` environment variable and the libraries within the folder indicated by `$HADOOP_HOME` will not be used.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="policyFilePath" optional="true" type="rstring">
          <description>
This optional parameter is relevant when connecting to BigInsights on Bluemix. It specifies the path to the directory that contains the Java Cryptography Extension policy files (US_export_policy.jar and local_policy.jar). The policy files enable the Java operators to use encryption with key sizes beyond the limits specified by the JDK. See the section on "Policy file configuration" in the main page of this toolkit's documentation for information on how to configure the policy files.
If this parameter is omitted the JVM default policy files will be used. When specified, this parameter takes precedence over the JVM default policy files.
**Note:** This parameter changes a JVM property. If you set this property, be sure it is set to the same value in all HDFS operators that are in the same PE. The location of the policy file directory can be absolute path on the file system or a path that is relative to the application directory.</description>
        </parameter>
        <inputPort maxNumPorts="1" minNumPorts="1" optional="true" windowPunctInputMode="Oblivious">
          <description>
The `HDFS2FileSource` operator has one optional input port. If an input port is specified, the operator expects
an input tuple with a single attribute of type rstring. The input tuples contain the file names that the operator opens for reading.
The input port is non-mutating.</description>
          <windowPolicy>NonWindowed</windowPolicy>
        </inputPort>
        <outputPort expressionMode="Nonexistent" maxNumPorts="1" minNumPorts="1" optional="false" windowPunctOutputMode="Generating">
          <description>
The `HDFS2FileSource` operator has one output port.  The tuples on the output port contain the data that is read from the files.
The operator supports two modes of reading.  To read a file line-by-line, the expected output schema of the output port is tuple&lt;rstring line&gt; or tuple&lt;ustring line&gt;.
To read a file as binary, the expected output schema of the output port is tuple&lt;blob data&gt;.  Use the blockSize parameter to control how much data to retrieve on each read.
The operator includes a punctuation marker at the conclusion of each file. The output port is mutating.</description>
        </outputPort>
        <codeTemplate name="HDFS2FileSource" template="stream&lt;${streamType}> ${streamName} = HDFS2FileSource() {&#xA;                param&#xA;                 file : &quot;${filename}&quot;;&#xA;                }">
          <description sampleUri="">HDFS2FileSource basic template</description>
        </codeTemplate>
        <codeTemplate name="HDFS2FileSource connecting to Bluemix" template="stream&lt;${streamType}> ${streamName} = HDFS2FileSource() {&#xA;                param&#xA;               &#x9; file: &quot;${filename}&quot;;&#xA;               &#x9; hdfsUser: &quot;${hdfsUser}&quot;;&#xA;               &#x9; hdfsPassword: &quot;${hdfsPassword}&quot;;&#xA;                 hdfsUri: &quot;${hdfsUri}&quot;;&#xA;                }">
          <description sampleUri="">Basic HDFS2FileSource template with required parameters for connecting to HDFS on Bluemix..</description>
        </codeTemplate>
      </primitiveOp>
      <primitiveOp language="Java" modelUriIndex="7" name="HDFS2FileSink" public="true">
        <description docHref="doc/${nl}/spldoc/html/tk$com.ibm.streamsx.hdfs/op$com.ibm.streamsx.hdfs$HDFS2FileSink.html">
The `HDFS2FileSink` operator writes files to a Hadoop Distributed File System. 

The `HDFS2FileSink` operator is similar to the `FileSink` operator.  
This operator writes tuples that arrive on its input port to the output file that is named by the **file** parameter.  
You can optionally control whether the operator closes the current output file and creates a new file for writing based on the size 
of the file in bytes, the number of tuples that are written to the file, or the time in seconds that the file is open for writing, 
or when the operator receives a punctuation marker.

# Behavior in a consistent region

The `HDFS2FileSink` operator can participate in a consistent region, however this is not supported when connecting to BigInsights on Bluemix.
The operator can be part of a consistent region, but cannot be at the start of a consistent region.
The operator guarantees that tuples are written to a file in HDFS at least once,
but duplicated tuples can be written to the file if application failure occurs.

For the operator to support consistent region, the Hadoop Distributed File System must be configured with file append
enabled. For information about how to properly enable this feature, refer to the documentation of your Hadoop distribution.

On drain, the operator flushes its internal buffer to the file.
On checkpoint, the operator stores the current file name, file size, tuple count, and file number to the checkpoint.
On reset, the operator closes the current file, and opens the file from checkpoint.
File states like file size and tuple count are reset to the file.
The file is opened in append mode, and data is written to the end of the file. 

# Exceptions

The `HDFS2FileSink` operator terminates in the following cases:
 * The operator cannot connect to HDFS.
 * The file cannot be written.     

+ Examples

This is a basic example using the `HDFS2FileSink` operator to write output to a Hadoop filesystem deployed on Bluemix.
    () as Sink= HDFS2FileSink(Input){
      param
        file   :"/user/biadmin/myfile.txt";
        hdfsUri:"webhdfs://server_host_name:port";   
        hdfsUser: "biadmin";
        hdfsPassword: "BluemixPassword";
    }   
    
    
This example uses the `HDFS2FileSink` operator to write tuples to output files that have names like `output0.txt`. SSL certificate validation is enabled.

    stream&lt;PersonSchema&gt; In = FileSource(){
      param
        file : "Input.txt" ;
    }
    stream&lt;rstring PersonSchemString&gt; SingleStringIn = Functor(In){
      output
        SingleStringIn : PersonSchemString =(rstring) In ;
    }
    () as txtSink = HDFS2FileSink(SingleStringIn){
      param
        file : "output%FILENUM.txt" ;
        bytesPerFile: (int64)(16*1024);
 		hdfsUri: "webhdfs://server_host_name:port";   
        hdfsUser: "biadmin";
        hdfsPassword: "BluemixPassword";
        keyStorePassword: "storepass";
        keyStorePath: "etc/store.jks";        
    }
</description>
        <images>
          <image size="32" uriIndex="8"/>
          <image size="16" uriIndex="9"/>
        </images>
        <parameter expressionMode="Constant" name="vmArg" optional="true" type="rstring">
          <description>
Specifies command-line arguments that are passed to the Java virtual machine that the operator runs within.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="file" optional="true" type="rstring">
          <description>
This parameter specifies the name of the file that the operator writes to.           
The **file** parameter can optionally contain the following variables, which the operator evaluates at runtime to generate the file name:
 * %HOST 		The host that is running the processing element (PE) of this operator.
 * %FILENUM		The file number, which starts at 0 and counts up as a new file is created for writing.
 * %PROCID		The process ID of the processing element.
 * %PEID 		The processing element ID.
 * %PELAUNCHNUM	The PE launch count.
 * %TIME 		The time when the file is created.  If the **timeFormat** parameter is not specified, the default time format is `yyyyMMdd_HHmmss`.
 
For example, if you specify a **file** parameter of `myFile%FILENUM%TIME.txt`, and the first three files are created in the afternoon on November 30, 2014, 
the file names are `myFile020141130_132443.txt`, `myfile120141130_132443.txt`, and `myFile220141130_132443.txt`.

**Important:** If the %FILENUM specification is not included, the file is overwritten every time a new file is created.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="timeFormat" optional="true" type="rstring">
          <description>
This parameter specifies the time format to use when the **file** parameter value contains `%TIME`.  
The parameter value must contain conversion specifications that are supported by the java.text.SimpleDateFormat. 
The default format is `yyyyMMdd_HHmmss`.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsUri" optional="true" type="rstring">
          <description>
This parameter specifies the uniform resource identifier (URI) that you can use to connect to the HDFS file system.  
The URI has the following format:
 * To access HDFS locally or remotely, use `hdfs://*hdfshost*:*hdfsport*`.
 * To access GPFS locally, use `gpfs:///`.
 * To access GPFS remotely, use `webhdfs://*hdfshost*:*webhdfsport*`.
 * To access HDFS via a web connection for HDFS deployed on Bluemix, use `webhdfs://webhdfshost:webhdfsport`. 
If this parameter is not specified, the operator expects that the HDFS URI is specified as the `fs.defaultFS` or `fs.default.name` 
property in the `core-site.xml` HDFS configuration file.  The operator expects the `core-site.xml` file to be located in 
`$HADOOP_HOME/../hadoop-conf` or `$HADOOP_HOME/etc/hadoop` or in the directory specified by the **configPath** parameter. 

You can use the **hdfsUri** parameter to override the value that is specified for the `fs.defaultFS` or `fs.default.name` option
in the `core-site.xml` configuration file.
**Note:** For applications connecting to HDFS deployed on Bluemix, the `$HADOOP_HOME` environment variable is not supported so either  **hdfsUri** or **configPath**  must be specified.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="bytesPerFile" optional="true" type="int64">
          <description>
This parameter specifies the approximate size of the output file, in bytes. 
When the file size exceeds the specified number of bytes, the current output file is closed and a new file is opened.  
The **bytesPerFile**, **timePerFile**, and **tuplesPerFile** parameters are mutually exclusive; you can specify only one of these parameters at a time.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="tuplesPerFile" optional="true" type="int64">
          <description>
This parameter specifies the maximum number of tuples that can be received for each output file. 
When the specified number of tuples are received, the current output file is closed and a new file is opened for writing. 
The **bytesPerFile**, **timePerFile**, and **tuplesPerFile** parameters are mutually exclusive; you can specify only one of these parameters at a time.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="closeOnPunct" optional="true" type="boolean">
          <description>
This parameter specifies whether the operator closes the current output file and creates a new file when a punctuation marker is received. 
The default value is `false`.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="fileAttributeName" optional="true" type="rstring">
          <description>If set, this points to an attribute containing the filename.  The operator will close a file when value of this attribute changes.  If the string contains substitutions, the check for a change happens before substituations, and the filename contains the substitutions based on the first tuple.    </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="timePerFile" optional="true" type="float64">
          <description>
This parameter specifies the approximate time, in seconds, after which the current output file is closed and a new file is opened for writing.  
The **bytesPerFile**, **timePerFile**, and **tuplesPerFile** parameters are mutually exclusive; you can specify only one of these parameters.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsUser" optional="true" type="rstring">
          <description>
This parameter specifies the user ID to use when you connect to the HDFS file system.  
If this parameter is not specified, the operator uses the instance owner ID to connect to the HDFS file system.  
When connecting to Hadoop instances on Bluemix, this parameter must be specified otherwise the connection will be unsuccessful.
When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by 
using the values that are specified in the **authPrincipal** and **authKeytab** parameters.  After successful authentication,
the operator uses the user ID that is specified in the **hdfsUser** parameter to perform all other operations on the file system.

**NOTE:** When you use Kerberos authentication, the InfoSphere Streams instance owner must have super user privileges on HDFS or GPFS
to perform operations as the user that is specified by the **hdfsUser** parameter.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="hdfsPassword" optional="true" type="rstring">
          <description>
This parameter specifies the password to use when you connecting to a Hadoop instance deployed on Bluemix.  
If this parameter is not specified, attempts to connect to a Hadoop instance deployed on Bluemix will cause an exception.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="keyStorePath" optional="true" type="rstring">
          <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on Bluemix. It specifies the path to the keystore file, which is in PEM format. The keystore file is used when making a secure connection to the HDFS server and must contain the public certificate of the HDFS server.  See the section on "SSL Configuration" in the main page of this toolkit's documentation for information on how to configure the keystore.
**Note: If this parameter is omitted, invalid certificates for secure connections will be accepted.**  If the keystore file does not exist, or if the certificate it contains is invalid, the operator terminates. The location of the keystore file can be absolute path on the file system or a path that is relative to the application directory.
        </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="keyStorePassword" optional="true" type="rstring">
          <description>This optional attribute is only supported when connecting to a Hadoop instance deployed on Bluemix. It specifies the password for the keystore file. This attribute is specified when the **keyStore** attribute is specified and the keystore file is protected by a password. If the keyStorePassword is invalid the operator terminates.
        </description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="authPrincipal" optional="true" type="rstring">
          <description>
This parameter specifies the Kerberos principal that you use for authentication. 
This value is set to the principal that is created for the InfoSphere Streams instance owner.
You must specify this parameter if you want to use Kerberos authentication.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="authKeytab" optional="true" type="rstring">
          <description>
This parameter specifies the file that contains the encrypted keys for the user that is specified by the **authPrincipal** parameter.
The operator uses this keytab file to authenticate the user.  The keytab file is generated by the administrator.
You must specify this parameter to use Kerberos authentication.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="credFile" optional="true" type="rstring">
          <description>
This parameter specifies the file that contains the login credentials. 
These credentials are used when you connect to GPFS remotely by using the `webhdfs://*hdfshost*:*hdfsport*` schema.
The credentials file must contain information on how to authenticate with IBM InfoSphere BigInsights when using the webhdfs schema.
For example, the file must contain the user name and password for an IBM InfoSphere BigInsights user.
When connecting to HDFS instances deployed on Bluemix, the credentials are provided using the **hdfsUser** and **hdfsPassword** parameters.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="configPath" optional="true" type="rstring">
          <description>
This parameter specifies the absolute path to the configuration directory that contains the `core-site.xml` file. 
If this parameter is not specified, by default the operator looks for the `core-site.xml` file in the following locations:
 * `$HADOOP_HOME/../hadoop-conf`
 * `$HADOOP_HOME/etc/hadoop`
 * `$HADOOP_HOME/conf`
 * `$HADOOP_HOME/share/hadoop/hdfs/\*`
 * `$HADOOP_HOME/share/hadoop/common/\*`
 * `$HADOOP_HOME/share/hadoop/common/lib/\*`
 * `$HADOOP_HOME/lib/\*`
 * `$HADOOP_HOME/\*`
You can use the **hdfsUri** parameter to override the value that is specified for the `fs.defaultFS` or `fs.default.name` option in the `core-site.xml` configuration file.
 **Note:** For connections to HDFS on Bluemix, the `$HADOOP_HOME` environment variable is not supported and so either  **hdfsUri** or **configPath**  must be specified.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="encoding" optional="true" type="rstring">
          <description>This parameter specifies the character set encoding that is used in the output le.</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="libPath" optional="true" type="rstring">
          <description>This optional parameter specifies the absolute path to the directory that contains the Hadoop library files. If this parameter is omitted and `$HADOOP_HOME` is not set, the Bluemix specific libraries within the `impl/lib/ext` folder of the toolkit will be used. 
When specified, this parameter takes precedence over the `$HADOOP_HOME` environment variable and the libraries within the folder indicated by `$HADOOP_HOME` will not be used.
</description>
        </parameter>
        <parameter cardinality="1" expressionMode="AttributeFree" name="policyFilePath" optional="true" type="rstring">
          <description>
This optional parameter is relevant when connecting to BigInsights on Bluemix. It specifies the path to the directory that contains the Java Cryptography Extension policy files (US_export_policy.jar and local_policy.jar). The policy files enable the Java operators to use encryption with key sizes beyond the limits specified by the JDK. See the section on "Policy file configuration" in the main page of this toolkit's documentation for information on how to configure the policy files.
If this parameter is omitted the JVM default policy files will be used. When specified, this parameter takes precedence over the JVM default policy files.
**Note:** This parameter changes a JVM property. If you set this property, be sure it is set to the same value in all HDFS operators that are in the same PE. The location of the policy file directory can be absolute path on the file system or a path that is relative to the application directory.</description>
        </parameter>
        <inputPort maxNumPorts="1" minNumPorts="1" optional="false" windowPunctInputMode="Oblivious">
          <description docHref="" sampleUri="">
The `HDFS2FileSink` operator has one input port, which writes the contents of the input stream to the file that you specified.
The input port is non-mutating, and its punctuation mode is `Oblivious`.  The HDFS2FileSink supports writing data into HDFS in two formats.
For line format, the schema of the input port is tuple&lt;rstring line&gt; or tuple&lt;ustring line&gt;, which specifies a single rstring or ustring attribute that represents 
a line to be written to the file. For binary format, the schema of the input port is tuple&lt;blob data&gt;, which specifies a block of data to be written to the file. </description>
          <windowPolicy>NonWindowed</windowPolicy>
        </inputPort>
        <outputPort expressionMode="Nonexistent" maxNumPorts="1" minNumPorts="1" optional="true" windowPunctOutputMode="Free">
          <description>
The `HDFS2FileSink` operator is configurable with an optional output port.  
The output port is non-mutating and its punctuation mode is `Free`.
The schema of the output port is &lt;string fileName, uint64 fileSize&gt;, which specifies the name and size of files that are written to HDFS.</description>
        </outputPort>
        <codeTemplate name="Basic HDFS2FileSink template" template="() as ${operatorName} = HDFS2FileSink(${inputStream} ) {&#xA;            param&#xA;                file: &quot;${filename}&quot;;&#xA;        }">
          <description sampleUri="">Basic HDFS2FileSink template</description>
        </codeTemplate>
        <codeTemplate name="HDFS2FileSink template for Bluemix" template="() as ${operatorName} = HDFS2FileSink(${inputStream} ) {&#xA;            param&#xA;                file: &quot;${filename}&quot;;&#xA;                hdfsUser: &quot;${hdfsUser}&quot;;&#xA;                hdfsUri: &quot;${hdfsUri}&quot;;&#xA;                hdfsPassword: &quot;${hdfsPassword}&quot;;&#xA;                &#xA;        }">
          <description sampleUri="">HDFS2FileSink template for Bluemix</description>
        </codeTemplate>
      </primitiveOp>
    </namespace>
    <sabFiles>
      <ti:include path="toolkit.xml" root="toolkitDir"/>
      <ti:include path="impl/java/lib/**" root="toolkitDir"/>
      <ti:include path="impl/java/bin/**" root="toolkitDir"/>
      <ti:include path="impl/bin/**" root="toolkitDir"/>
      <ti:include path="impl/lib/**" root="toolkitDir"/>
      <ti:include path="impl/nl/*.dat" root="toolkitDir"/>
      <ti:include path="etc/**" root="toolkitDir"/>
      <ti:include path="lib/**" root="toolkitDir"/>
      <ti:include path="nl/**" root="toolkitDir"/>
      <ti:include path="opt/**" root="toolkitDir"/>
    </sabFiles>
  </toolkit>

</toolkitModel>
