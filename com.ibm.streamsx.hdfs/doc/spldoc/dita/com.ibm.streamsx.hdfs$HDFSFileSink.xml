<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//ENs"
"/homes/hny7/chanskw/streams/ext/dita-ot/dtd/technicalContent/dtd/reference.dtd">
<reference id ="spldoc_operator__com.ibm.streamsx.hdfs__HDFSFileSink">
<title>com.ibm.streamsx.hdfs::HDFSFileSink</title>
<refbody>
<section>
<p>
<xref href="toolkit.xml">
com.ibm.streamsx.hdfs
</xref>
-&gt; <xref href="com.ibm.streamsx.hdfs$.xml">com.ibm.streamsx.hdfs</xref>-&gt; HDFSFileSink</p>
</section>
<section>

<p>The HDFSFileSink operator writes files to a Hadoop Distributed File System.  
</p>
<p>The HDFSFileSink operator is similar to the FileSink operator.  This operator writes tuples that arrive on its input port to the output file that is named by the <b>file</b> parameter.   You can optionally control whether the operator closes the current output file and creates a new file for writing based on the size of the file in bytes, the number of tuples that are written to the file, or the time in seconds that the file is open for writing, or when the operator receives a punctuation marker. 
</p>
<p><b>Exceptions</b> 
</p>
<p>The HDFSFileSink operator terminates in the following cases: 
</p>
<ul>
<li> The operator cannot connect to HDFS. </li>
<li> The file cannot be written.      </li>
</ul>
<p> <b>Examples</b> 
</p>
<p>The following example shows how to use the HDFSFileSink operator: 
</p>
<p>
<codeblock>
<![CDATA[  stream<PersonSchema> In = FileSource(){
  			param
			file : "Input.txt" ;
	}
	stream<rstring PersonSchemString> SingleStringIn = Functor(In){
  			output
			SingleStringIn : PersonSchemString =(rstring) In ;
	}
	() as txtSink = HDFS2FileSink(SingleStringIn){
  			param
			file : "output%FILENUM.txt" ;
			bytesPerFile: (int64)(16*1024);
	}
]]></codeblock>

</p>
</section>
<section outputclass="splprimop">
<object type="image/svg+xml" data="../image/com.ibm.streamsx.hdfs$HDFSFileSink.svg" width="672" height="112">
<desc>Primitive operator image not displayed. Problem loading file: ../image/com.ibm.streamsx.hdfs$HDFSFileSink.svg
</desc>
</object>
</section>
<section>
<title outputclass="splhead-1">Summary</title>
<dl compact="yes">
 <dlentry>
  <dt>Ports</dt>
  <dd>This operator has 1 input port and 1 output port.</dd>
 </dlentry>
 <dlentry>
  <dt>Windowing</dt>
  <dd>This operator does not accept any windowing configurations.</dd>
 </dlentry>
 <dlentry>
  <dt>Parameters</dt>
  <dd>This operator supports 13 parameters. (<tt>file, timeFormat, hdfsUri, bytesPerFile, tuplesPerFile, closeOnPunct, timePerFile, hdfsUser, authPrincipal, authKeytab, credFile, configPath, encoding</tt>)
</dd>
 </dlentry>
 <dlentry>
  <dt>Metrics</dt>
  <dd>This operator does not report any metrics.</dd>
 </dlentry>
</dl></section>
<section>
<title outputclass="splhead-1">Properties</title>
<dl compact="yes">
 <dlentry>
  <dt>Implementation</dt>
  <dd>Java</dd>
 </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set">Input Ports</xref></b></p><dl>
  <dlentry>
   <dt>Ports (0)</dt>
   <dd>

<p>The HDFSFileSink operator has one input port, which writes the contents of the input stream to the file that you specified. The input port is non-mutating, and its punctuation mode is <tt>Oblivious</tt>. The schema of the input port is tuple&lt;rstring line&gt; or tuple&lt;ustring line&gt;, which specifies a single rstring or ustring attribute that represents  a line to be written to the file.
</p>   </dd>
  </dlentry>
    <dlentry>
      <dt>Windowing</dt>
      <dd>
      </dd>
    </dlentry>
    <dlentry>
      <dt>Properties</dt>
      <dd>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__optional">Optional</xref>: false
     </sli>
   </sl>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__controlport">ControlPort</xref>: false
</sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__windowingmode">WindowingMode</xref>: NonWindowed
</sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__windowpunctuationinputmode">WindowPunctuationInputMode</xref>: Oblivious
</sli>
   </sl>
      </dd>
    </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/output_port_set">Output Ports</xref></b></p><dl>
  <dlentry>
     <dt>Assignments</dt>
       <dd>Java operators do not support output assignments.
       </dd>
  </dlentry>
</dl>
<dl>
  <dlentry>
   <dt>Ports (0)</dt>
   <dd>

<p>The HDFSFileSink operator is configurable with an optional output port.   The output port is non-mutating and its punctuation mode is <tt>Free</tt>. The schema of the output port is &lt;string fileName, uint64 fileSize&gt;, which specifies the name and size of files that are written to HDFS.
</p>
<p></p>
   <dl>
    <dlentry>
      <dt>Properties</dt>
      <dd>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/output_port_set__optional">Optional</xref>: true
     </sli>
   </sl>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/output_port_set__windowpunctuationoutputmode">WindowPunctuationOutputMode</xref>: Free
     </sli>
   </sl>
      </dd>
    </dlentry>
   </dl>
   <p></p>
   </dd>
  </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters">Parameters</xref></b></p>
<dl>
<dlentry>
<dt id="parameter_file"><tt>file</tt></dt>
<dd>
<p>This parameter specifies the name of the file that the operator writes to.            The <b>file</b> parameter can optionally contain the following variables, which the operator evaluates at runtime to generate the file name: 
</p>
<ul>
<li> %HOST 		The host that is running the processing element (PE) of this operator. </li>
<li> %FILENUM		The file number, which starts at 0 and counts up as a new file is created for writing. </li>
<li> %PROCID		The process ID of the processing element. </li>
<li> %PEID 		The processing element ID. </li>
<li> %PELAUNCHNUM	The PE launch count. </li>
<li> %TIME 		The time when the file is created.  If the <b>timeFormat</b> parameter is not specified, the default time format is <tt>yyyyMMdd_HHmmss</tt>.   </li>
</ul>
<p>For example, if you specify a <b>file</b> parameter of <tt>myFile%FILENUM%TIME.txt</tt>, and the first three files are created in the afternoon on November 30, 2014,  the file names are <tt>myFile020141130_132443.txt</tt>, <tt>myfile120141130_132443.txt</tt>, and <tt>myFile220141130_132443.txt</tt>. 
</p>
<p><b>Important:</b> If the %FILENUM specification is not included, the file is overwritten every time a new file is created.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: false
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_timeFormat"><tt>timeFormat</tt></dt>
<dd>
<p>This parameter specifies the time format to use when the <b>file</b> parameter value contains <tt>%TIME</tt>.   The parameter value must contain conversion specifications that are supported by the java.text.SimpleDateFormat.  The default format is <tt>yyyyMMdd_HHmmss</tt>.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_hdfsUri"><tt>hdfsUri</tt></dt>
<dd>
<p>This parameter specifies the uniform resource identifier (URI) that you can use to connect to the HDFS file system.   The URI has the following format: 
</p>
<ul>
<li> To access HDFS locally or remotely, use <tt>hdfs://*hdfshost*:*hdfsport*</tt>. </li>
<li> To access GPFS locally, use <tt>gpfs:///</tt>. </li>
<li> To access GPFS remotely, use <tt>webhdfs://*hdfshost*:*webhdfsport*</tt>.   </li>
</ul>
<p>If this parameter is not specified, the operator expects that the HDFS URI is specified as the <tt>fs.defaultFS</tt> or <tt>fs.default.name</tt>  property in the <tt>core-site.xml</tt> HDFS configuration file.  The operator expects the <tt>core-site.xml</tt> file to be located in  <tt>$HADOOP_HOME/../hadoop-conf</tt> or <tt>$HADOOP_HOME/etc/hadoop</tt>.  
</p>
<p>You can use the <b>hdfsUri</b> parameter to override the value that is specified for the <tt>fs.defaultFS</tt> or <tt>fs.default.name</tt> option in the <tt>core-site.xml</tt> configuration file.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_bytesPerFile"><tt>bytesPerFile</tt></dt>
<dd>
<p>This parameter specifies the approximate size of the output file, in bytes.  When the file size exceeds the specified number of bytes, the current output file is closed and a new file is opened.   The <b>bytesPerFile</b>, <b>timePerFile</b>, and <b>tuplesPerFile</b> parameters are mutually exclusive; you can specify only one of these parameters at a time.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: int64
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_tuplesPerFile"><tt>tuplesPerFile</tt></dt>
<dd>
<p>This parameter specifies the maximum number of tuples that can be received for each output file.  When the specified number of tuples are received, the current output file is closed and a new file is opened for writing.  The <b>bytesPerFile</b>, <b>timePerFile</b>, and <b>tuplesPerFile</b> parameters are mutually exclusive; you can specify only one of these parameters at a time.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: int64
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_closeOnPunct"><tt>closeOnPunct</tt></dt>
<dd>
<p>This parameter specifies whether the operator closes the current output file and creates a new file when a punctuation marker is received.  The default value is <tt>false</tt>.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: boolean
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_timePerFile"><tt>timePerFile</tt></dt>
<dd>
<p>This parameter specifies the approximate time, in seconds, after which the current output file is closed and a new file is opened for writing.   The <b>bytesPerFile</b>, <b>timePerFile</b>, and <b>tuplesPerFile</b> parameters are mutually exclusive; you can specify only one of these parameters.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: float64
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_hdfsUser"><tt>hdfsUser</tt></dt>
<dd>
<p>This parameter specifies the user ID to use when you connect to the HDFS file system.   If this parameter is not specified, the operator uses the instance owner ID to connect to the HDFS file system. 
</p>
<p>When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by  using the values that are specified in the <b>authPrincipal</b> and <b>authKeytab</b> parameters.  After successful authentication, the operator uses the user ID that is specified in the <b>hdfsUser</b> parameter to perform all other operations on the file system. 
</p>
<p><b>NOTE:</b> When you use Kerberos authentication, the InfoSphere Streams instance owner must have super user privileges on HDFS or GPFS to perform operations as the user that is specified by the <b>hdfsUser</b> parameter.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_authPrincipal"><tt>authPrincipal</tt></dt>
<dd>
<p>This parameter specifies the Kerberos principal that you use for authentication.  This value is set to the principal that is created for the InfoSphere Streams instance owner. You must specify this parameter if you want to use Kerberos authentication.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_authKeytab"><tt>authKeytab</tt></dt>
<dd>
<p>This parameter specifies the file that contains the encrypted keys for the user that is specified by the <b>authPrincipal</b> parameter. The operator uses this keytab file to authenticate the user.  The keytab file is generated by the administrator. You must specify this parameter to use Kerberos authentication.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_credFile"><tt>credFile</tt></dt>
<dd>
<p>This parameter specifies the file that contains the login credentials.  These credentials are used when you connect to GPFS remotely by using the <tt>webhdfs://*hdfshost*:*hdfsport*</tt> schema. The credentials file must contain information on how to authenticate with IBM InfoSphere BigInsights when using the webhdfs schema. For example, the file must contain the user name and password for an IBM InfoSphere BigInsights user.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_configPath"><tt>configPath</tt></dt>
<dd>
<p>This parameter specifies the absolute path to the configuration directory that contains the <tt>core-site.xml</tt> file.  If this parameter is not specified, the operator searches the default location for the <tt>core-site.xml</tt>.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_encoding"><tt>encoding</tt></dt>
<dd>
<p>This parameter specifies the character set encoding that is used in the output Ô¨Åle.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__codetemplate">Code Templates</xref></b></p>  <dl>
    <dlentry>
     <dt>HDFSFileSink</dt>
     <dd>
       <codeblock>
<![CDATA[() as ${operatorName} = HDFSFileSink(${inputStream} ) {
            param
                file: "${filename}";
        }]]>
      </codeblock>
     <p></p>
     </dd>
    </dlentry>
    <dlentry>
     <dt>HDFSFileSink with hdfsUser and hdfsUri</dt>
     <dd>
       <codeblock>
<![CDATA[() as ${operatorName} = HDFSFileSink(${inputStream} ) {
            param
                file: "${filename}";
                hdfsUser: "${hdfsUser}";
                hdfsUri: "${hdfsUri}";
        }]]>
      </codeblock>
     <p></p>
     </dd>
    </dlentry>
  </dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__dependencies">Libraries</xref></b></p>
 <dl>
 <dlentry>
  <dt>Java operator class library
  </dt>
  <dd/>
  <dd><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__dependencies__managedlibrary__libpath">Library Path</xref>: <tt>../../impl/lib/BigData.jar, ../../impl/java/bin</tt></dd>
 </dlentry>
 <dlentry>
  <dt>apache library
  </dt>
  <dd/>
  <dd><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__dependencies__managedlibrary__libpath">Library Path</xref>: <tt>@HADOOP_HOME@/../hadoop-conf, @HADOOP_HOME@/etc/hadoop, @HADOOP_HOME@/conf, @HADOOP_HOME@/share/hadoop/hdfs/*, @HADOOP_HOME@/share/hadoop/common/*, @HADOOP_HOME@/share/hadoop/common/lib/*, @HADOOP_HOME@/lib/*, @HADOOP_HOME@/client/*, @HADOOP_HOME@/*, @HADOOP_HOME@/../hadoop-hdfs/*</tt></dd>
 </dlentry>
 </dl>
</section>
</refbody>
</reference>

