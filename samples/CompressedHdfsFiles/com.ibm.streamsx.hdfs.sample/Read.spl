namespace com.ibm.streamsx.hdfs.sample;

use com.ibm.streamsx.hdfs::HDFS2FileSource ;

/**
 * Read compressed data from HDFS with the Decompress and Parse operators.
 * 
 * Before running this composite, run the Write composite to write 
 * data into HDFS.  
 * 
 * Once the file <code>testCompress.gz</code> exists, this application
 * will read the file and output the results in csv format.
 * 
 * The steps are:
 * * Read the binary data from HDFS
 * * Decompress the data blocks.
 * * Parse the data blocks into tuples
 * 
 * We then write the tuples to disk so that you can verify they are what we compressed
 * in the first place. 
 */
composite Read
{
	param
	expression<rstring> $hdfsUri : getSubmissionTimeValue("hdfsUri", "hdfs://9.30.253.149:8020");
	graph
		stream<blob compressedBlock> Compressed =
			HDFS2FileSource()
		{
			param
				file : "testCompress.gz" ;
				hdfsUri : $hdfsUri;
		}

		stream<blob uncompressedBlock> Decompressed =
			Decompress(Compressed)
		{
			param
				compression : gzip ;
		}

		stream<rstring message, int32 anInt, float64 aFloat> Parsed = Parse(Decompressed)
		{
			param
				format : csv ;
		}

		() as sink= FileSink(Parsed)
		{
			logic
				onTuple Parsed : 
					printStringLn(message + " " + (rstring) anInt + " " + (rstring) aFloat);					
			param
				file : "readUncompressParse.csv" ;
		}

}

