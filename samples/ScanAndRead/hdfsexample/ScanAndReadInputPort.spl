/*******************************************************************************
* Copyright (C) 2014, International Business Machines Corporation
* All Rights Reserved
*******************************************************************************/                                
namespace hdfsexample ;

use com.ibm.streamsx.hdfs::HDFS2DirectoryScan ;
use com.ibm.streamsx.hdfs::HDFS2FileSource ;

/**
 * The [ScanAndReadWithInputPort] composite demonstrates how you can make use of the 
 * optional input port from HDFS2DirectoryScan to dynamically modify the directory to scan at runtime.
 *
 * To build this application, you need to set up the HADOOP_HOME environment variable as follows:
 * * export HADOOP_HOME=<Hadoop installation>
 * 
 * Setup:
 * You will need to create 2 new directories for the HDFS2DirectoryScan operator to scan.
 * 1. Create a new directory in HDFS by running this command at the command line:
 *  * <HADOOP_HOME>/bin/hadoop fs -mkdir /user/<userid>/testDirectory
 * 1. Copy a few files from local file system to the testDirectory on HDFS:
 *  * <HADOOP_HOME>/bin/hadoop fs -copyFromLocal <localFile> /user/<userid>/testDirectory
 * 1. Create a second directory named "newDirectory" in HDFS and also copy a few files into that directory.  
 * 
 * To compile and run this sample in Streams Studio:
 * 1. Start Streams Studio
 * 1. In Streams Explorer view, add the "com.ibm.streams.bigdata" toolkit as one of the toolkit locations
 * 1. From the main menu, select File -> Import.
 * 1. Expand InfoSphere Streams Studio
 * 1. Select Sample SPL Application
 * 1. Select this sample from the Import SPL Sample Application dialog
 * 1. Once the sample is imported, wait for the build to finish.  If autobuild is turned off, select resulting project, right click -> Build Project
 * 1. Once the project is built, select the main composite of the sample, right click -> Launch Active Build Config *  
 * 
 * To compile and run this sample at the commandn line:
 * If compiling and running from command line, follow these steps:
 *  1. Create a directory. For example, you can create a directory in your home directory. 
 *   * mkdir $HOME/hdfssamples
 *  1. Copy the samples to this directory. For example, you can copy the samples to the directory created above. 
 *   * cp -R $STREAMS_INSTALL/toolkits/com.ibm.streams.bigdata/samples/* $HOME/hdfssamples/
 *  1. Build one of the sample applications. Go to the appropriate subdirectory and run the make. By default, the sample is compiled as a distributed application. If you want to compile the application as a standalone application, run make standalone instead. Run make clean to return the samples back to their original state.
 *  1. Run the sample application. 
 *   * To run the ScanAndReadWithInputPort sample application in distributed mode, Start your IBM InfoSphere Streams instance, then use the streamtool command to submit the .adl files that were generated during the application build. 
 *    * streamtool submitjob -i <instance_name> output/Distributed/hdfsexample::ScanAndReadWithInputPort/hdfsexample.ScanAndReadWithInputPort.adl -P hdfsUri="hdfs://<machine_name>:<port>" -P hdfsUser="<user_name>"
 *   * To run the ScanAndReadWithInputPort sample application in standalone mode, issue the command:
 *    * ./output/Standalone/hdfsexample::ScanAndReadWithInputPort/bin/standalone hdfsUri="hdfs://<machine_name>:<port>" hdfsUser="<user_name>"
 *
 * @param hdfsUri URI to HDFS.  If unspecified, the operator expects that the HDFS URI is specified as the fs.defaultFS or fs.default.name property in core-site.xml.
 * @param hdfsUser User to connect to HDFS.  If unspecified the instance owner running the application will be used as the user
 * to log onto the HDFS. Example value: "streamsadmin".
 */
composite ScanAndReadWithInputPort
{
	param
		expression<rstring> $hdfsUri : getSubmissionTimeValue("hdfsUri", "hdfs://10.6.25.164:8020"); //format hdfs://host:port
		expression<rstring> $hdfsUser : getSubmissionTimeValue("hdfsUser", "hbase"); 

	graph

		//Custom operator that sends a control signal to HDFS2DirectoryScan operator
		stream<rstring newDir> ControlSignalStream = Custom()
		{
			logic
				onProcess :
				{
					mutable int32 cnt = 0 ;
					while(true)
					{
						block(20.0f) ;
						if(cnt == 0)
						{
							tuple<rstring newDir> newDirTuple = { newDir = "newDirectory" } ;
							submit(newDirTuple, ControlSignalStream) ;
						}

						cnt ++ ;
					}

				}
		}

		// Scan /user/<userid>/testDirectory for 20 seconds
		// When the custom operator sends in the control signal, the directory
		// scan will start scanning /user/<userid>/newDirectory.
		stream<rstring fileNames> FileNameStream1 =
			HDFS2DirectoryScan(ControlSignalStream)
		{
			param
				directory	: "testDirectory" ;
				hdfsUri		: $hdfsUri;
				hdfsUser	: $hdfsUser;
	}
		
		// read file discovered by directory scan
		stream<rstring lines> LineStream = HDFS2FileSource(FileNameStream1)
		{
			param
				hdfsUri		: $hdfsUri;
				hdfsUser	: $hdfsUser;
		} 

		// write content of files to "FileContentsInputPort.txt" in sample's data directory
		() as LineSink1 = FileSink(LineStream)
		{
			param
				file : "FileContentsInputPort.txt" ;
		}

}
