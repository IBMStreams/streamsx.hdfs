/*******************************************************************************
* Copyright (C) 2014, International Business Machines Corporation
* All Rights Reserved
*******************************************************************************/                                
namespace hdfsexample ;

use com.ibm.streamsx.hdfs::HDFS2DirectoryScan ;
use com.ibm.streamsx.hdfs::HDFS2FileSource ;

/**
 * The [ScanAndReadWithInputPort] composite demonstrates how you can make use of the 
 * optional input port from HDFS2DirectoryScan to dynamically modify the directory to scan at runtime.
 *
 * To build this application, you need to set up the HADOOP_HOME environment variable as follows:
 * * export HADOOP_HOME=<Hadoop installation>
 * 
 * Setup:
 * You will need to create 2 new directories for the HDFS2DirectoryScan operator to scan.
 * 1. Create a new directory in HDFS by running this command at the command line:
 *  * <HADOOP_HOME>/bin/hadoop fs -mkdir /user/<userid>/testDirectory
 * 1. Copy a few files from local file system to the testDirectory on HDFS:
 *  * <HADOOP_HOME>/bin/hadoop fs -copyFromLocal <localFile> /user/<userid>/testDirectory
 * 1. Create a second directory named "newDirectory" in HDFS and also copy a few files into that directory.  
 * 
 * To compile and run this sample in Streams Studio:
 * 1. Start Streams Studio
 * 1. In Streams Explorer view, add the "com.ibm.streams.bigdata" toolkit as one of the toolkit locations
 * 1. From the main menu, select File -> Import.
 * 1. Expand IBM Streams Studio
 * 1. Select Sample SPL Application
 * 1. Select this sample from the Import SPL Sample Application dialog
 * 1. Once the sample is imported, wait for the build to finish.  If autobuild is turned off, select resulting project, right click -> Build Project
 * 1. Once the project is built, select the main composite of the sample, right click -> Launch Active Build Config *  
 * 
 * To compile and run this sample at the commandn line:
 * If compiling and running from command line, follow these steps:
 *  1. Create a directory. For example, you can create a directory in your home directory. 
 *   * mkdir $HOME/hdfssamples
 *  1. Copy the samples to this directory. For example, you can copy the samples to the directory created above. 
 *   * cp -R $STREAMS_INSTALL/toolkits/com.ibm.streams.bigdata/samples/* $HOME/hdfssamples/
 *  1. Build one of the sample applications. Go to the appropriate subdirectory and run the make. By default, the sample is compiled as a distributed application. If you want to compile the application as a standalone application, run make standalone instead. Run make clean to return the samples back to their original state.
 *  1. Run the sample application. 
 *   * To run the ScanAndReadWithInputPort sample application in distributed mode, Start your IBM Streams instance, then use the streamtool command to submit the .adl files that were generated during the application build. 
 *    * streamtool submitjob -i <instance_name> output/Distributed/hdfsexample::ScanAndReadWithInputPort/hdfsexample.ScanAndReadWithInputPort.adl -P hdfsUri="hdfs://<machine_name>:<port>" -P hdfsUser="<user_name>"
 *   * To run the ScanAndReadWithInputPort sample application in standalone mode, issue the command:
 *    * ./output/Standalone/hdfsexample::ScanAndReadWithInputPort/bin/standalone hdfsUri="hdfs://<machine_name>:<port>" hdfsUser="<user_name>"
 *
 * @param hdfsUri URI to HDFS.  If unspecified, the operator expects that the HDFS URI is specified as the fs.defaultFS or fs.default.name property in core-site.xml.
 * @param hdfsUser User to connect to HDFS.  If unspecified the instance owner running the application will be used as the user
 * to log onto the HDFS. Example value: "streamsadmin".
 */
composite ScanAndReadWithInputPort
{
	param
		expression<rstring> $hdfsUri : getSubmissionTimeValue("hdfsUri", "hdfs://10.6.25.164:8020"); //format hdfs://host:port
		expression<rstring> $hdfsUser : getSubmissionTimeValue("hdfsUser", "hbase"); 

	graph

		//Custom operator that sends a control signal to HDFS2DirectoryScan operator
		stream<rstring newDir> ControlSignalStream = Custom()
		{
			logic
				onProcess :
				{
					mutable int32 cnt = 0 ;
					while(true)
					{
						block(20.0f) ;
						if(cnt == 0)
						{
							tuple<rstring newDir> newDirTuple = { newDir = "newDirectory" } ;
							submit(newDirTuple, ControlSignalStream) ;
						}

						cnt ++ ;
					}

				}
		}

		// Scan /user/<userid>/testDirectory for 20 seconds
		// When the custom operator sends in the control signal, the directory
		// scan will start scanning /user/<userid>/newDirectory.
		stream<rstring fileNames> FileNameStream1 =
			HDFS2DirectoryScan(ControlSignalStream)
		{
			param
				directory	: "testDirectory" ;
				hdfsUri		: $hdfsUri;
				hdfsUser	: $hdfsUser;
	}
		
		// read file discovered by directory scan
		stream<rstring lines> LineStream = HDFS2FileSource(FileNameStream1)
		{
			param
				hdfsUri		: $hdfsUri;
				hdfsUser	: $hdfsUser;
		} 

		// write content of files to "FileContentsInputPort.txt" in sample's data directory
		() as LineSink1 = FileSink(LineStream)
		{
			param
				file : "FileContentsInputPort.txt" ;
		}

}
