<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="Toolkit com.ibm.streamsx.hdfs 4.5.0"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_toolkit"/>
<link rel="stylesheet" type="text/css" href="../../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../../html/spldoc.css"/>
<title>Toolkit com.ibm.streamsx.hdfs 4.5.0</title>
</head>
<body id="spldoc_toolkit">


<h1 class="title topictitle1 spltoolkitname">Toolkit <tt class="ph tt">com.ibm.streamsx.hdfs 4.5.0</tt></h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="../toolkits/toolkits.html">IBMStreams com.ibm.streamsx.hdfs Toolkit</a> &gt; com.ibm.streamsx.hdfs 4.5.0</p>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">General Information</h2>


<p class="p">The <strong class="ph b">streamsx.hdfs</strong> toolkit provides operators that can read and write data from Hadoop Distributed File System <strong class="ph b">(HDFS)</strong> version 2 or later. It support also copy files from local file system to the remote HDFS and from HDFS to the local file system. 
</p>

<p class="p"><strong class="ph b">HDFS2FileSource</strong>     This operator opens a file on HDFS and sends out its contents in tuple format on its output port.  
</p>

<p class="p"><strong class="ph b">HDFS2FileSink</strong>       This operator writes tuples that arrive on its input port to the output file that is named by the <strong class="ph b">file</strong> parameter.   
</p>

<p class="p"><strong class="ph b">HDFS2DirectoryScan</strong>  This operator repeatedly scans an HDFS directory and writes the names of new or modified files that are found in the directory to the output port. The operator sleeps between scans.
</p>

<p class="p"><strong class="ph b">HDFS2FileCopy</strong>       This operator copies files from local file system to the remote HDFS and from HDFS to the local disk.
</p>

<div class="p">The operators in this toolkit use Hadoop Java APIs to access HDFS and WEBHDFS. The operators support the following versions of Hadoop distributions:
<ul class="ul">
<li class="li"> Apache Hadoop versions 2.7 </li>

<li class="li"> Apache Hadoop versions 3.0 or higher</li>

<li class="li"> Cloudera distribution including Apache Hadoop version 4 (CDH4) and version 5 (CDH 5)</li>

<li class="li"> Hortonworks Data Platform (HDP) Version 2.6 or higher</li>

<li class="li"> Hortonworks Data Platform (HDP) Version 3.0 or higher</li>

<li class="li"> IBM Analytic Engine 1.1 (HDP 2.7)</li>

<li class="li"> IBM Analytic Engine 1.2 (HDP 3.1)</li>

</ul>

</div>

<p class="p">Note: The reference platforms that were used for testing are Hadoop 2.7.3, HDP .
</p>

<p class="p">You can access Hadoop remotely by specifying the <tt class="ph tt">webhdfs://hdfshost:webhdfsport</tt> schema in the URI that you use to connect to GPFS.
</p>

<p class="p">For example:
</p>

<div class="p">
<pre class="pre codeblock">
  () as lineSink1 = HDFS2FileSink(LineIn)
  {  
         param
              hdfsUri       : "clsadmin": 
              hdfsUser      : "webhdfs://your-hdfs-host-ip-address:8443";
              hdfsPassword  : "PASSWORD";
              file          : "LineInput.txt" ;
              
  }
</pre>


</div>

<p class="p"> Or "hdfs://your-hdfs-host-ip-address:8020" as hdfsUri
</p>

<div class="p">
<pre class="pre codeblock">
  () as lineSink1 = HDFS2FileSink(LineIn)
  {  
         param
              hdfsUri  : "hdfs": 
              hdfsUser : "hdfs://your-hdfs-host-ip-address:8020"
              file     : "LineInput.txt" ;                  
  }
</pre>


</div>

<p class="p"> This example copies the file test.txt from local path ./data/ into /user/hdfs/works directory of HDFS.   The parameter <strong class="ph b">credentials</strong> is a JSON string that contains <tt class="ph tt">user</tt>, <tt class="ph tt">password</tt> and <tt class="ph tt">webhdsf</tt>.
</p>

<div class="p">
<pre class="pre codeblock">
  streams&lt;boolean succeed&gt; copyFromLocal =  HDFS2FileCopy()
  {
         param
              localFile                : "test.txt"; 
              hdfsFile                 : "/user/hdfs/works/test.txt"; 
              deleteSourceFile         : false; 
              overwriteDestinationFile : true; 
              direction                : copyFromLocalFile;
              credentials              : $credentials ;
  }
</pre>


</div>

<p class="p"><strong class="ph b">Kerberos configuration</strong> 
</p>

<p class="p">For Apache Hadoop 2.x, CDH, and HDP, you can optionally configure these operators to use the Kerberos protocol to authenticate users that read and write to HDFS. 
</p>

<p class="p">Kerberos authentication provides a more secure way of accessing HDFS by providing user authentication. 
</p>

<p class="p">To use Kerberos authentication, you must configure the <strong class="ph b">authPrincipal</strong> and <strong class="ph b">authKeytab</strong> operator parameters at compile time. 
</p>

<p class="p">The <strong class="ph b">authPrincipal</strong> parameter specifies the Kerberos principal, which is typically the principal that is created for the Streams instance owner. 
</p>

<p class="p">The <strong class="ph b">authKeytab</strong> parameter specifies the keytab file that is created for the principal.
</p>

<p class="p">For <strong class="ph b">Kerberos</strong> authentication it is required to create a <strong class="ph b">Principal</strong> and a <strong class="ph b">Keytab</strong> for each user.
</p>

<p class="p">If you use <strong class="ph b">ambari</strong> to configure your hadoop server, you can create principals and keytabs via ambari (Enable Kerberos).
</p>

<p class="p">More details about Kerberos configuration: 
</p>

<div class="p">
<pre class="pre codeblock">
  https://developer.ibm.com/hadoop/2016/08/18/overview-of-kerberos-in-iop-4-2/
</pre>


</div>

<p class="p">Copy the created keytab into local streams server for example in etc directory of your SPL application.
</p>

<p class="p">Before you start your SPL application, you can check the keytab with <strong class="ph b">kinit</strong> tool
</p>

<div class="p">
<pre class="pre codeblock">
  kinit -k -t KeytabPath Principal
</pre>


</div>

<p class="p">KeytabPath is the full path to the keytab file
</p>

<p class="p">For example: 
</p>

<div class="p">
<pre class="pre codeblock">
  kinit -k -t /home/streamsadmin/workspace/myproject/etc/hdfs.headless.keytab hdfs-hdp2@HDP2.COM
</pre>


</div>

<p class="p">In this case <strong class="ph b">HDP2.com</strong> is the <strong class="ph b">kerebors realm</strong> and the user is <strong class="ph b">hdfs</strong>.
</p>

<p class="p">Here is an SPL example to write a file into hadoop server with kerberos configuration. 
</p>

<div class="p">
<pre class="pre codeblock">
    () as lineSink1 = HDFS2FileSink(LineIn)
    {
        param
            authKeytab     : "etc/hdfs.headless.keytab" ;
            authPrincipal  : "hdfs-hdp2@HDP2.COM" ;
            configPath     : "etc" ;
            file           : "LineInput.txt" ;
    }
</pre>


</div>

<p class="p"> The HDSF configuration file <strong class="ph b">core-site.xml</strong>  has to be copied into local <tt class="ph tt">etc</tt> directory.
</p>

  <dl class="dl"><dt class="dt dlterm"><a class="xref" href="tk$com.ibm.streamsx.hdfs$1.html">Developing and running applications that use the HDFS Toolkit</a>
</dt>
<dd class="dd">
</dd>
</dl>

<dl class="dl">
  
  <dt class="dt dlterm">Version</dt>

  <dd class="dd">4.5.0</dd>

  
  
  <dt class="dt dlterm">Required Product Version</dt>

  <dd class="dd">4.2.0.0</dd>

  
  
  <dt class="dt dlterm">Author</dt>

  <dd class="dd">IBMStreams Open Source Community at GitHub - https://github.com/IBMStreams/streamsx.hdfs</dd>

  
</dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Indexes</h2>
  
  <dl class="dl">
    <dt class="dt dlterm"/>
<dd class="dd"/>

    <dt class="dt dlterm"><a class="xref" href="ix$Namespace.html">Namespaces</a></dt>
<dd class="dd"/>

    <dt class="dt dlterm"><a class="xref" href="ix$Operator.html">Operators</a></dt>
<dd class="dd"/>

  </dl>

</div>

<div class="section"><h2 class="title sectiontitle splhead-1">Namespaces</h2>
  
  <dl class="dl">
    
      <dt class="dt dlterm splpart"><a class="xref" href="ns$com.ibm.streamsx.hdfs.html">com.ibm.streamsx.hdfs</a></dt>

      <dd class="dd"/>

      <dd class="dd"><dl class="dl">
        <dt class="dt dlterm">Operators</dt>

        <dd class="dd">
<ul class="sl simple">
<li class="sli"><a class="xref" href="op$com.ibm.streamsx.hdfs$HDFS2DirectoryScan.html">HDFS2DirectoryScan</a></li>

<li class="sli"><a class="xref" href="op$com.ibm.streamsx.hdfs$HDFS2FileCopy.html">HDFS2FileCopy</a></li>

<li class="sli"><a class="xref" href="op$com.ibm.streamsx.hdfs$HDFS2FileSink.html">HDFS2FileSink</a></li>

<li class="sli"><a class="xref" href="op$com.ibm.streamsx.hdfs$HDFS2FileSource.html">HDFS2FileSource</a></li>

</ul>

        </dd>

      </dl>
</dd>

       </dl>

</div>

</div>


</body>
</html>